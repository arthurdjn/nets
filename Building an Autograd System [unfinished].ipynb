{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Buil an Autograd System From Scratch with NumPy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Set Up\n",
    "\n",
    "Let's load the packages we need to run our tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random as rd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# This is a bit of magic to make matplotlib figures appear inline in the notebook\n",
    "# rather than in a new window.\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "# Change the font ?\n",
    "font = {'size'   : 15}\n",
    "plt.rc('font', **font)\n",
    "\n",
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's fix the seed\n",
    "\n",
    "SEED = 42\n",
    "rd.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Basic Operations\n",
    "\n",
    "## 1.1. With PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t1 =\n",
      "tensor([[1, 3],\n",
      "        [5, 7]])\n",
      "t2 =\n",
      "tensor([[2, 4],\n",
      "        [6, 8]])\n",
      "\n",
      "Some basic operations\n",
      "t1 + t2: \n",
      "tensor([[ 3,  7],\n",
      "        [11, 15]])\n",
      "t1 - t2: \n",
      "tensor([[-1, -1],\n",
      "        [-1, -1]])\n",
      "t1 * t2: \n",
      "tensor([[ 2, 12],\n",
      "        [30, 56]])\n",
      "t1 @ t2: \n",
      "tensor([[20, 28],\n",
      "        [52, 76]])\n",
      "t1 ** 2: \n",
      "tensor([[ 1,  9],\n",
      "        [25, 49]])\n",
      "t1 / t2: \n",
      "tensor([[0, 0],\n",
      "        [0, 0]])\n",
      "t1 / 10: \n",
      "tensor([[0, 0],\n",
      "        [0, 0]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.tensor([[1, 3], \n",
    "                  [5, 7]])\n",
    "t2 = torch.tensor([[2, 4], \n",
    "                  [6, 8]])\n",
    "\n",
    "print(f\"t1 =\\n{t1}\")\n",
    "print(f\"t2 =\\n{t2}\")\n",
    "\n",
    "print(\"\\nSome basic operations\")\n",
    "print(f\"t1 + t2: \\n{torch.add(t1, t2)}\")\n",
    "print(f\"t1 - t2: \\n{torch.sub(t1, t2)}\")\n",
    "print(f\"t1 * t2: \\n{torch.mul(t1, t2)}\")\n",
    "print(f\"t1 @ t2: \\n{t1 @ t2}\")\n",
    "print(f\"t1 ** 2: \\n{torch.pow(t1, 2)}\")\n",
    "print(f\"t1 / t2: \\n{torch.div(t1, t2)}\")\n",
    "print(f\"t1 / 10: \\n{t1 / 10}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t1 =\n",
      "tensor([[1 3]\n",
      "        [5 7]])\n",
      "t2 =\n",
      "tensor([[2 4]\n",
      "        [6 8]])\n",
      "\n",
      "Some basic operations\n",
      "t1 + t2: \n",
      "tensor([[ 3  7]\n",
      "        [11 15]])\n",
      "t1 - t2: \n",
      "tensor([[-1 -1]\n",
      "        [-1 -1]])\n",
      "t1 * t2: \n",
      "tensor([[ 2 12]\n",
      "        [30 56]])\n",
      "t1 @ t2: \n",
      "tensor([[20 28]\n",
      "        [52 76]])\n",
      "t1 ** 2: \n",
      "tensor([[ 1  9]\n",
      "        [25 49]])\n",
      "t1 / t2: \n",
      "tensor([[0.5    0.75  ]\n",
      "        [0.8333 0.875 ]])\n",
      "t1 / 10: \n",
      "tensor([[0.1 0.3]\n",
      "        [0.5 0.7]])\n"
     ]
    }
   ],
   "source": [
    "import nets\n",
    "\n",
    "t1 = nets.Tensor([[1, 3], \n",
    "                  [5, 7]])\n",
    "t2 = nets.Tensor([[2, 4], \n",
    "                  [6, 8]])\n",
    "\n",
    "print(f\"t1 =\\n{t1}\")\n",
    "print(f\"t2 =\\n{t2}\")\n",
    "\n",
    "print(\"\\nSome basic operations\")\n",
    "print(f\"t1 + t2: \\n{nets.add(t1, t2)}\")\n",
    "print(f\"t1 - t2: \\n{nets.sub(t1, t2)}\")\n",
    "print(f\"t1 * t2: \\n{nets.multiply(t1, t2)}\")\n",
    "print(f\"t1 @ t2: \\n{nets.dot(t1, t2)}\")\n",
    "print(f\"t1 ** 2: \\n{nets.pow(t1, 2)}\")\n",
    "print(f\"t1 / t2: \\n{nets.div(t1, t2)}\")\n",
    "print(f\"t1 / 10: \\n{t1 / 10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Autograd\n",
    "\n",
    "\n",
    "**NETS** uses a custom autograd system, made with numpy. Some vanilla architectures do not depends on this functionality however, like ``DNN`` networks.\n",
    "All these informations will be detailed in the model's section.\n",
    "\n",
    "As you may have seen, there is a ``requires_grad`` set to ``False`` by default when we create a tensor. This attribute attributes works similarly as **PyTorch**'s attribute. If set to ``True``, previous gradients will be registered and saved in this tensor, in the ``_hooks`` attribute. This attribute is basically a list containing all previous gradients. That is, when calling the ``backward`` method on this tensor with an upstream gradient, it will propagate through all previous gradients.\n",
    "\n",
    "## 2.1. With Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t1 =\n",
      "tensor([1., 3.], requires_grad=True)\n",
      "t2 =\n",
      "tensor([2., 4.], requires_grad=True)\n",
      "\n",
      "Operation:\n",
      "t3 = t1 + t2 + 4\n",
      "t4 = t3 * t2\n",
      "\n",
      "t3 =\n",
      "tensor([ 7., 11.], grad_fn=<AddBackward0>)\n",
      "t4 =\n",
      "tensor([14., 44.], grad_fn=<MulBackward0>)\n",
      "\n",
      "Before backpropagation\n",
      "t1 gradient: None\n",
      "t2 gradient: None\n",
      "t3 gradient: None\n",
      "t4 gradient: None\n",
      "\n",
      "After backpropagation\n",
      "t1 gradient: tensor([-2.,  8.])\n",
      "t2 gradient: tensor([-9., 30.])\n",
      "t3 gradient: None\n",
      "t4 gradient: None\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.tensor([1., 3], requires_grad=True)\n",
    "t2 = torch.tensor([2., 4], requires_grad=True)\n",
    "\n",
    "# Some operations\n",
    "t3 = t1 + t2 + 4\n",
    "t4 = t3 * t2\n",
    "\n",
    "print(f\"t1 =\\n{t1}\")\n",
    "print(f\"t2 =\\n{t2}\")\n",
    "\n",
    "print(\"\\nOperation:\")\n",
    "print(\"t3 = t1 + t2 + 4\")\n",
    "print(\"t4 = t3 * t2\")\n",
    "\n",
    "print(f\"\\nt3 =\\n{t3}\")\n",
    "print(f\"t4 =\\n{t4}\")\n",
    "\n",
    "print(\"\\nBefore backpropagation\")\n",
    "print(f\"t1 gradient: {t1.grad}\")\n",
    "print(f\"t2 gradient: {t2.grad}\")\n",
    "print(f\"t3 gradient: {t3.grad}\")\n",
    "print(f\"t4 gradient: {t4.grad}\")\n",
    "\n",
    "# Upstream gradient\n",
    "grad = torch.tensor([-1., 2.])\n",
    "\n",
    "# Back-propagation\n",
    "t4.backward(grad)\n",
    "\n",
    "print(\"\\nAfter backpropagation\")\n",
    "print(f\"t1 gradient: {t1.grad}\")\n",
    "print(f\"t2 gradient: {t2.grad}\")\n",
    "print(f\"t3 gradient: {t3.grad}\")\n",
    "print(f\"t4 gradient: {t4.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. With NETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t1 =\n",
      "tensor([1 3], requires_grad=True)\n",
      "t2 =\n",
      "tensor([2 4], requires_grad=True)\n",
      "\n",
      "Operation:\n",
      "t3 = t1 + t2 + 4\n",
      "t4 = t3 * t2\n",
      "\n",
      "t3 =\n",
      "tensor([ 7 11], requires_grad=True)\n",
      "t4 =\n",
      "tensor([14 44], requires_grad=True)\n",
      "\n",
      "Before backpropagation\n",
      "t1 gradient: tensor([0. 0.])\n",
      "t2 gradient: tensor([0. 0.])\n",
      "t3 gradient: tensor([0. 0.])\n",
      "t4 gradient: tensor([0. 0.])\n",
      "\n",
      "After backpropagation\n",
      "t1 gradient: tensor([-2.  8.])\n",
      "t2 gradient: tensor([-9. 30.])\n",
      "t3 gradient: tensor([-2.  8.])\n",
      "t4 gradient: tensor([-1.  2.])\n"
     ]
    }
   ],
   "source": [
    "t1 = nets.Tensor([1, 3], requires_grad=True)\n",
    "t2 = nets.Tensor([2, 4], requires_grad=True)\n",
    "\n",
    "# Some operations\n",
    "t3 = t1 + t2 + 4\n",
    "t4 = t3 * t2\n",
    "\n",
    "print(f\"t1 =\\n{t1}\")\n",
    "print(f\"t2 =\\n{t2}\")\n",
    "\n",
    "print(\"\\nOperation:\")\n",
    "print(\"t3 = t1 + t2 + 4\")\n",
    "print(\"t4 = t3 * t2\")\n",
    "\n",
    "print(f\"\\nt3 =\\n{t3}\")\n",
    "print(f\"t4 =\\n{t4}\")\n",
    "\n",
    "print(\"\\nBefore backpropagation\")\n",
    "print(f\"t1 gradient: {t1.grad}\")\n",
    "print(f\"t2 gradient: {t2.grad}\")\n",
    "print(f\"t3 gradient: {t3.grad}\")\n",
    "print(f\"t4 gradient: {t4.grad}\")\n",
    "\n",
    "# Upstream gradient\n",
    "grad = nets.Tensor([-1, 2])\n",
    "\n",
    "# Back-propagation\n",
    "t4.backward(grad)\n",
    "\n",
    "print(\"\\nAfter backpropagation\")\n",
    "print(f\"t1 gradient: {t1.grad}\")\n",
    "print(f\"t2 gradient: {t2.grad}\")\n",
    "print(f\"t3 gradient: {t3.grad}\")\n",
    "print(f\"t4 gradient: {t4.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differences so far"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Advance operations and autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. With PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t1 =\n",
      "tensor([[1., 3.],\n",
      "        [5., 7.]], requires_grad=True)\n",
      "t2 =\n",
      "tensor([[2., 4.],\n",
      "        [6., 8.]], requires_grad=True)\n",
      "\n",
      "Before backpropagation\n",
      "t1 gradient:\n",
      "None\n",
      "t2 gradient:\n",
      "None\n",
      "t3 = tanh(t1 * t2) gradient:\n",
      "None\n",
      "t4 = t3 ** 3 + t2 gradient:\n",
      "None\n",
      "t5 = exp(t4) gradient:\n",
      "None\n",
      "t6 = t5.T gradient:\n",
      "None\n",
      "t7 = log(t6) gradient:\n",
      "None\n",
      "\n",
      "After backpropagation\n",
      "t1 gradient:\n",
      "tensor([[-7.1306,  0.0000],\n",
      "        [-0.0000,  0.0000]])\n",
      "t2 gradient:\n",
      "tensor([[  -21.6654,   296.8263],\n",
      "        [-1096.6332, 16206.1680]])\n",
      "t3 = tanh(t1 * t2) gradient:\n",
      "None\n",
      "t4 = t3 ** 3 + t2 gradient:\n",
      "None\n",
      "t5 = exp(t4) gradient:\n",
      "None\n",
      "t6 = t5.T gradient:\n",
      "None\n",
      "t7 = log(t6) gradient:\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.tensor([[1., 3.],\n",
    "                   [5., 7.]], requires_grad=True)\n",
    "t2 = torch.tensor([[2., 4.],\n",
    "                   [6., 8.]], requires_grad=True)\n",
    "\n",
    "# Some operations\n",
    "t3 = torch.tanh(t1 * t2)\n",
    "t4 = t3 ** 3 + t2\n",
    "t5 = torch.exp(t4)\n",
    "t6 = t5.T\n",
    "t7 = torch.exp(torch.log(t6))\n",
    "\n",
    "print(f\"t1 =\\n{t1}\")\n",
    "print(f\"t2 =\\n{t2}\")\n",
    "\n",
    "print(\"\\nBefore backpropagation\")\n",
    "print(f\"t1 gradient:\\n{t1.grad}\")\n",
    "print(f\"t2 gradient:\\n{t2.grad}\")\n",
    "print(f\"t3 = tanh(t1 * t2) gradient:\\n{t3.grad}\")\n",
    "print(f\"t4 = t3 ** 3 + t2 gradient:\\n{t4.grad}\")\n",
    "print(f\"t5 = exp(t4) gradient:\\n{t5.grad}\")\n",
    "print(f\"t6 = t5.T gradient:\\n{t6.grad}\")\n",
    "print(f\"t7 = log(t6) gradient:\\n{t7.grad}\")\n",
    "\n",
    "\n",
    "# Upstream gradient\n",
    "grad = torch.tensor([[-1., 2.],\n",
    "                     [-3., 4.]])\n",
    "\n",
    "# Back-propagation\n",
    "t7.sum(axis=1).backward(grad[0])\n",
    "\n",
    "print(\"\\nAfter backpropagation\")\n",
    "print(f\"t1 gradient:\\n{t1.grad}\")\n",
    "print(f\"t2 gradient:\\n{t2.grad}\")\n",
    "print(f\"t3 = tanh(t1 * t2) gradient:\\n{t3.grad}\")\n",
    "print(f\"t4 = t3 ** 3 + t2 gradient:\\n{t4.grad}\")\n",
    "print(f\"t5 = exp(t4) gradient:\\n{t5.grad}\")\n",
    "print(f\"t6 = t5.T gradient:\\n{t6.grad}\")\n",
    "print(f\"t7 = log(t6) gradient:\\n{t7.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. With NETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t1 =\n",
      "tensor([[1. 3.]\n",
      "        [5. 7.]], requires_grad=True)\n",
      "t2 =\n",
      "tensor([[2. 4.]\n",
      "        [6. 8.]], requires_grad=True)\n",
      "\n",
      "Before backpropagation\n",
      "t1 gradient:\n",
      "tensor([[0. 0.]\n",
      "        [0. 0.]])\n",
      "t2 gradient:\n",
      "tensor([[0. 0.]\n",
      "        [0. 0.]])\n",
      "t3 = tanh(t1 * t2) gradient:\n",
      "tensor([[0. 0.]\n",
      "        [0. 0.]])\n",
      "t4 = t3 ** 3 + t2 gradient:\n",
      "tensor([[0. 0.]\n",
      "        [0. 0.]])\n",
      "t5 = exp(t4) gradient:\n",
      "tensor([[0. 0.]\n",
      "        [0. 0.]])\n",
      "t6 = t5.T gradient:\n",
      "tensor([[0. 0.]\n",
      "        [0. 0.]])\n",
      "t7 = log(t6) gradient:\n",
      "tensor([[0. 0.]\n",
      "        [0. 0.]])\n",
      "\n",
      "After backpropagation\n",
      "t1 gradient:\n",
      "tensor([[-7.1306e+00  5.3787e-07]\n",
      "        [ 0.0000e+00  0.0000e+00]])\n",
      "t2 gradient:\n",
      "tensor([[  -21.6654   296.8263]\n",
      "        [-1096.6332 16206.1679]])\n",
      "t3 = tanh(t1 * t2) gradient:\n",
      "tensor([[  -50.464    890.479 ]\n",
      "        [-3289.8995 48618.5036]])\n",
      "t4 = t3 ** 3 + t2 gradient:\n",
      "tensor([[  -18.1001   296.8263]\n",
      "        [-1096.6332 16206.1679]])\n",
      "t5 = exp(t4) gradient:\n",
      "tensor([[-1.  2.]\n",
      "        [-1.  2.]])\n",
      "t6 = t5.T gradient:\n",
      "tensor([[-1. -1.]\n",
      "        [ 2.  2.]])\n",
      "t7 = log(t6) gradient:\n",
      "tensor([[-1. -1.]\n",
      "        [ 2.  2.]])\n"
     ]
    }
   ],
   "source": [
    "t1 = nets.Tensor([[1., 3.],\n",
    "                   [5., 7.]], requires_grad=True)\n",
    "t2 = nets.Tensor([[2., 4.],\n",
    "                   [6., 8.]], requires_grad=True)\n",
    "\n",
    "# Some operations\n",
    "t3 = nets.tanh(t1 * t2)\n",
    "t4 = t3 ** 3 + t2\n",
    "t5 = nets.exp(t4)\n",
    "t6 = t5.T\n",
    "t7 = nets.exp(nets.log(t6))\n",
    "\n",
    "print(f\"t1 =\\n{t1}\")\n",
    "print(f\"t2 =\\n{t2}\")\n",
    "\n",
    "print(\"\\nBefore backpropagation\")\n",
    "print(f\"t1 gradient:\\n{t1.grad}\")\n",
    "print(f\"t2 gradient:\\n{t2.grad}\")\n",
    "print(f\"t3 = tanh(t1 * t2) gradient:\\n{t3.grad}\")\n",
    "print(f\"t4 = t3 ** 3 + t2 gradient:\\n{t4.grad}\")\n",
    "print(f\"t5 = exp(t4) gradient:\\n{t5.grad}\")\n",
    "print(f\"t6 = t5.T gradient:\\n{t6.grad}\")\n",
    "print(f\"t7 = log(t6) gradient:\\n{t7.grad}\")\n",
    "\n",
    "# Upstream gradient\n",
    "grad = nets.Tensor([[-1., 2.],\n",
    "                    [-3., 4.]])\n",
    "\n",
    "# Back-propagation\n",
    "t7.sum(axis=1).backward(grad[0])\n",
    "\n",
    "print(\"\\nAfter backpropagation\")\n",
    "print(f\"t1 gradient:\\n{t1.grad}\")\n",
    "print(f\"t2 gradient:\\n{t2.grad}\")\n",
    "print(f\"t3 = tanh(t1 * t2) gradient:\\n{t3.grad}\")\n",
    "print(f\"t4 = t3 ** 3 + t2 gradient:\\n{t4.grad}\")\n",
    "print(f\"t5 = exp(t4) gradient:\\n{t5.grad}\")\n",
    "print(f\"t6 = t5.T gradient:\\n{t6.grad}\")\n",
    "print(f\"t7 = log(t6) gradient:\\n{t7.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So far so good !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Even more advanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. With PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Before backpropagation\n",
      "t1:\n",
      "tensor([[[-0.1255,  0.4507,  0.2320],\n",
      "         [ 0.0987, -0.3440, -0.3440],\n",
      "         [-0.4419,  0.3662,  0.1011]],\n",
      "\n",
      "        [[ 0.2081, -0.4794,  0.4699],\n",
      "         [ 0.3324, -0.2877, -0.3182],\n",
      "         [-0.3166, -0.1958,  0.0248]],\n",
      "\n",
      "        [[-0.0681, -0.2088,  0.1119],\n",
      "         [-0.3605, -0.2079, -0.1336],\n",
      "         [-0.0439,  0.2852, -0.3003]]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "t2:\n",
      "tensor([[[ 0.0142,  0.0924, -0.4535],\n",
      "         [ 0.1075, -0.3295, -0.4349],\n",
      "         [ 0.4489,  0.4656,  0.3084]],\n",
      "\n",
      "        [[-0.1954, -0.4023,  0.1842],\n",
      "         [-0.0598, -0.3780, -0.0048],\n",
      "         [-0.4656,  0.4093, -0.2412]],\n",
      "\n",
      "        [[ 0.1625, -0.1883,  0.0201],\n",
      "         [ 0.0467, -0.3151,  0.4696],\n",
      "         [ 0.2751,  0.4395,  0.3948]]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "t3:\n",
      "tensor([[[-0.0125,  0.4507,  0.2320],\n",
      "         [ 0.0987, -0.0344, -0.0344],\n",
      "         [-0.0442,  0.3662,  0.1011]],\n",
      "\n",
      "        [[ 0.2081, -0.0479,  0.4699],\n",
      "         [ 0.3324, -0.0288, -0.0318],\n",
      "         [-0.0317, -0.0196,  0.0248]],\n",
      "\n",
      "        [[-0.0068, -0.0209,  0.1119],\n",
      "         [-0.0361, -0.0208, -0.0134],\n",
      "         [-0.0044,  0.2852, -0.0300]]], dtype=torch.float64,\n",
      "       grad_fn=<LeakyReluBackward0>)\n",
      "\n",
      "After backpropagation\n",
      "t1 gradient:\n",
      "tensor([[[ 0.0098,  0.4219, -0.4115],\n",
      "         [-0.3040, -0.0455, -0.0175],\n",
      "         [-0.0111, -0.2287,  0.3287]],\n",
      "\n",
      "        [[-0.1432, -0.0219,  0.0427],\n",
      "         [-0.3591,  0.0302, -0.0425],\n",
      "         [ 0.0487,  0.0272, -0.3013]],\n",
      "\n",
      "        [[-0.0494,  0.0315,  0.2069],\n",
      "         [ 0.0229,  0.0271, -0.0426],\n",
      "         [-0.0142, -0.3841,  0.0363]]], dtype=torch.float64)\n",
      "t2 gradient:\n",
      "None\n",
      "t3 gradient:\n",
      "None\n",
      "grad\n",
      " tensor([[[ 0.0979,  0.4219, -0.4115],\n",
      "         [-0.3040, -0.4548, -0.1747],\n",
      "         [-0.1113, -0.2287,  0.3287]],\n",
      "\n",
      "        [[-0.1432, -0.2191,  0.0427],\n",
      "         [-0.3591,  0.3022, -0.4254],\n",
      "         [ 0.4869,  0.2722, -0.3013]],\n",
      "\n",
      "        [[-0.4945,  0.3155,  0.2069],\n",
      "         [ 0.2290,  0.2713, -0.4260],\n",
      "         [-0.1415, -0.3841,  0.3631]]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "data1 = np.random.rand(3, 3, 3) - 0.5\n",
    "data2 = np.random.rand(3, 3, 3) - 0.5\n",
    "grad = np.random.rand(3, 3, 3) - 0.5\n",
    "\n",
    "t1 = torch.tensor(data1, requires_grad=True)\n",
    "t2 = torch.tensor(data2, requires_grad=True)\n",
    "\n",
    "# Some operations\n",
    "t3 = torch.nn.functional.leaky_relu(t1, 0.1)\n",
    "\n",
    "print(\"\\nBefore backpropagation\")\n",
    "print(f\"t1:\\n{t1}\")\n",
    "print(f\"t2:\\n{t2}\")\n",
    "print(f\"t3:\\n{t3}\")\n",
    "\n",
    "\n",
    "# Upstream gradient\n",
    "grad = torch.tensor(grad)\n",
    "\n",
    "# Back-propagation\n",
    "t3.backward(grad)\n",
    "\n",
    "print(\"\\nAfter backpropagation\")\n",
    "print(f\"t1 gradient:\\n{t1.grad}\")\n",
    "print(f\"t2 gradient:\\n{t2.grad}\")\n",
    "print(f\"t3 gradient:\\n{t3.grad}\")\n",
    "print('grad\\n', grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. With NETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Before backpropagation\n",
      "t1:\n",
      "tensor([[[-0.1255  0.4507  0.232 ]\n",
      "         [ 0.0987 -0.344  -0.344 ]\n",
      "         [-0.4419  0.3662  0.1011]]\n",
      "\n",
      "        [[ 0.2081 -0.4794  0.4699]\n",
      "         [ 0.3324 -0.2877 -0.3182]\n",
      "         [-0.3166 -0.1958  0.0248]]\n",
      "\n",
      "        [[-0.0681 -0.2088  0.1119]\n",
      "         [-0.3605 -0.2079 -0.1336]\n",
      "         [-0.0439  0.2852 -0.3003]]], requires_grad=True)\n",
      "t2:\n",
      "tensor([[[ 0.0142  0.0924 -0.4535]\n",
      "         [ 0.1075 -0.3295 -0.4349]\n",
      "         [ 0.4489  0.4656  0.3084]]\n",
      "\n",
      "        [[-0.1954 -0.4023  0.1842]\n",
      "         [-0.0598 -0.378  -0.0048]\n",
      "         [-0.4656  0.4093 -0.2412]]\n",
      "\n",
      "        [[ 0.1625 -0.1883  0.0201]\n",
      "         [ 0.0467 -0.3151  0.4696]\n",
      "         [ 0.2751  0.4395  0.3948]]], requires_grad=True)\n",
      "t3:\n",
      "tensor([[[-0.0125  0.4507  0.232 ]\n",
      "         [ 0.0987 -0.0344 -0.0344]\n",
      "         [-0.0442  0.3662  0.1011]]\n",
      "\n",
      "        [[ 0.2081 -0.0479  0.4699]\n",
      "         [ 0.3324 -0.0288 -0.0318]\n",
      "         [-0.0317 -0.0196  0.0248]]\n",
      "\n",
      "        [[-0.0068 -0.0209  0.1119]\n",
      "         [-0.0361 -0.0208 -0.0134]\n",
      "         [-0.0044  0.2852 -0.03  ]]], requires_grad=True)\n",
      "\n",
      "After backpropagation\n",
      "t1 gradient:\n",
      "tensor([[[ 0.0098  0.4219 -0.4115]\n",
      "         [-0.304  -0.0455 -0.0175]\n",
      "         [-0.0111 -0.2287  0.3287]]\n",
      "\n",
      "        [[-0.1432 -0.0219  0.0427]\n",
      "         [-0.3591  0.0302 -0.0425]\n",
      "         [ 0.0487  0.0272 -0.3013]]\n",
      "\n",
      "        [[-0.0494  0.0315  0.2069]\n",
      "         [ 0.0229  0.0271 -0.0426]\n",
      "         [-0.0142 -0.3841  0.0363]]])\n",
      "t2 gradient:\n",
      "tensor([[[0. 0. 0.]\n",
      "         [0. 0. 0.]\n",
      "         [0. 0. 0.]]\n",
      "\n",
      "        [[0. 0. 0.]\n",
      "         [0. 0. 0.]\n",
      "         [0. 0. 0.]]\n",
      "\n",
      "        [[0. 0. 0.]\n",
      "         [0. 0. 0.]\n",
      "         [0. 0. 0.]]])\n",
      "t3 gradient:\n",
      "tensor([[[ 0.0979  0.4219 -0.4115]\n",
      "         [-0.304  -0.4548 -0.1747]\n",
      "         [-0.1113 -0.2287  0.3287]]\n",
      "\n",
      "        [[-0.1432 -0.2191  0.0427]\n",
      "         [-0.3591  0.3022 -0.4254]\n",
      "         [ 0.4869  0.2722 -0.3013]]\n",
      "\n",
      "        [[-0.4945  0.3155  0.2069]\n",
      "         [ 0.229   0.2713 -0.426 ]\n",
      "         [-0.1415 -0.3841  0.3631]]])\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "data1 = np.random.rand(3, 3, 3) - 0.5\n",
    "data2 = np.random.rand(3, 3, 3) - 0.5\n",
    "grad = np.random.rand(3, 3, 3) - 0.5\n",
    "\n",
    "t1 = nets.Tensor(data1, requires_grad=True)\n",
    "t2 = nets.Tensor(data2, requires_grad=True)\n",
    "\n",
    "# Some operations\n",
    "t3 = nets.leaky_relu(t1, 0.1)\n",
    "\n",
    "print(\"\\nBefore backpropagation\")\n",
    "print(f\"t1:\\n{t1}\")\n",
    "print(f\"t2:\\n{t2}\")\n",
    "print(f\"t3:\\n{t3}\")\n",
    "\n",
    "\n",
    "# Upstream gradient\n",
    "grad = nets.Tensor(grad)\n",
    "\n",
    "# Back-propagation\n",
    "t3.backward(grad)\n",
    "\n",
    "print(\"\\nAfter backpropagation\")\n",
    "print(f\"t1 gradient:\\n{t1.grad}\")\n",
    "print(f\"t2 gradient:\\n{t2.grad}\")\n",
    "print(f\"t3 gradient:\\n{t3.grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
