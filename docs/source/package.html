


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>nets &mdash; nets 0.1 documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../_static/pygments.css" type="text/css" /> -->
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="prev" title="Convolutional Layers" href="tutorial-cnn.html" /> 

  
  <script src="../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://arthurdjn.github.io/nets/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://arthurdjn.github.io/nets/source/getting-started.html">Get Started</a>
          </li>

          <li>
            <a href="https://arthurdujardin.com/project/nets.html">Blog</a>
          </li>

          <li>
            <a href="https://arthurdjn.github.io/nets/source/tutorial.html">Tutorials</a>
          </li>

          <li>
            <a href="https://arthurdjn.github.io/nets/">Docs</a>
          </li>

          <li>
            <a href="https://github.com/arthurdjn/nets">Github</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>


<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            

            
              
              
            

            


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

            
          </div>

          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="getting-started.html">About</a></li>
<li class="toctree-l1"><a class="reference internal" href="getting-started.html#instalation">Instalation</a></li>
<li class="toctree-l1"><a class="reference internal" href="getting-started.html#usage">Usage</a></li>
</ul>
<p class="caption"><span class="caption-text">Tutorial</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="tutorial.html">Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorial-xor.html">XOR</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorial-cnn.html">Convolutional Layers</a></li>
</ul>
<p class="caption"><span class="caption-text">Package</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">nets</a></li>
<li class="toctree-l1"><a class="reference internal" href="#id1">nets</a></li>
<li class="toctree-l1"><a class="reference internal" href="#nets-autograd">nets.autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="#nets-nn">nets.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="#nets-nn-modules">nets.nn.modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="#nets-optim">nets.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="#nets-solver">nets.solver</a></li>
<li class="toctree-l1"><a class="reference internal" href="#nets-data">nets.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="#nets-datasets">nets.datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="#nets-utils">nets.utils</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
      <li>nets</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="../_sources/source/package.rst.txt" rel="nofollow"><img src="../_static/images/view-page-source-icon.svg"></a>
          
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <div class="section" id="nets">
<h1>nets<a class="headerlink" href="#nets" title="Permalink to this headline">¶</a></h1>
<p><strong>NETS</strong> is divided in five main sub-packages:</p>
<ul class="simple">
<li><p><a class="reference external" href="nets.autograd">autograd</a> refers to the autograd system used,</p></li>
<li><p><a class="reference external" href="nets.nn">nn</a> refers to the core of the package, <em>ie</em> deep network models,</p></li>
<li><p><a class="reference external" href="nets.optim">optim</a> refers to optimizers used during the backpropagation to update weights and biases,</p></li>
<li><p><a class="reference external" href="nets.data">data</a> refers to array container, like training and testing examples with their labels,</p></li>
<li><p><a class="reference external" href="nets.datasets">datasets</a> refers to popular machine learning libraries for image analysis and NLP.</p></li>
</ul>
</div>
<div class="section" id="id1">
<h1>nets<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h1>
<div class="section" id="module-nets.tensor">
<span id="nets-tensor"></span><h2>nets.tensor<a class="headerlink" href="#module-nets.tensor" title="Permalink to this headline">¶</a></h2>
<p>Defines tensors for deep learning application. A tensor is multi-dimensional array, similar to <code class="docutils literal notranslate"><span class="pre">numpy</span></code> arrays.</p>
<dl class="class">
<dt id="nets.tensor.Tensor">
<em class="property">class </em><code class="sig-prename descclassname">nets.tensor.</code><code class="sig-name descname">Tensor</code><span class="sig-paren">(</span><em class="sig-param">data</em>, <em class="sig-param">requires_grad=False</em>, <em class="sig-param">hooks=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/tensor.html#Tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.tensor.Tensor" title="Permalink to this definition">¶</a></dt>
<dd><p>A Tensor is a multi dimensional array that track and record previous gradients, creating a dynamic
computational graph.</p>
<dl class="method">
<dt id="nets.tensor.Tensor.append">
<code class="sig-name descname">append</code><span class="sig-paren">(</span><em class="sig-param">t</em>, <em class="sig-param">axis=0</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/tensor.html#Tensor.append"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.tensor.Tensor.append" title="Permalink to this definition">¶</a></dt>
<dd><p>Append a value(- or tensor) to a <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The operation takes place in-place and does not support autograd.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>t</strong> (<em>scale</em><em>, </em><a class="reference internal" href="#nets.tensor.Tensor" title="nets.tensor.Tensor"><em>Tensor</em></a>) – object to add</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>None</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="nets.tensor.Tensor.astype">
<code class="sig-name descname">astype</code><span class="sig-paren">(</span><em class="sig-param">new_type</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/tensor.html#Tensor.astype"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.tensor.Tensor.astype" title="Permalink to this definition">¶</a></dt>
<dd><p>Set a new type to the <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>’s data.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>new_type</strong> (<em>type</em>) – new type to convert the data</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>None</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="nets.tensor.Tensor.backward">
<code class="sig-name descname">backward</code><span class="sig-paren">(</span><em class="sig-param">grad=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/tensor.html#Tensor.backward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.tensor.Tensor.backward" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute a single backward pass on all <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> linked to this one.
The <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> depending to this top-level <code class="docutils literal notranslate"><span class="pre">Tensor``are</span> <span class="pre">stored</span> <span class="pre">in</span> <span class="pre">the</span> <span class="pre">``_hooks</span></code> attribute.
The backward pass compute a gradient back-propagation on all <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> registered in <code class="docutils literal notranslate"><span class="pre">_hooks</span></code>.
The backward pass gradient in <code class="docutils literal notranslate"><span class="pre">grad</span></code> attribute (and add upstream gradient if the <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>
is used multiple times).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>grad</strong> (<a class="reference internal" href="#nets.tensor.Tensor" title="nets.tensor.Tensor"><em>Tensor</em></a>) – upstream gradient. Default is None, and will be set to <code class="docutils literal notranslate"><span class="pre">Tensor(1.0)</span></code>, a 0-dimensional
<code class="docutils literal notranslate"><span class="pre">Tensor</span></code>.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>None</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>To be able to back-propagate, the top-level <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> must have <code class="docutils literal notranslate"><span class="pre">requires_grad</span></code> set to <code class="docutils literal notranslate"><span class="pre">True</span></code>
to propagate the gradient.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="nets.tensor.Tensor.detach">
<code class="sig-name descname">detach</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/tensor.html#Tensor.detach"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.tensor.Tensor.detach" title="Permalink to this definition">¶</a></dt>
<dd><p>Unlink the <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> to the computational graph.
By calling this method, the attribute <code class="docutils literal notranslate"><span class="pre">_hooks</span></code> and <code class="docutils literal notranslate"><span class="pre">grad</span></code> are set to their default values,
<code class="docutils literal notranslate"><span class="pre">None</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>None</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="nets.tensor.Tensor.flatten">
<code class="sig-name descname">flatten</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/tensor.html#Tensor.flatten"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.tensor.Tensor.flatten" title="Permalink to this definition">¶</a></dt>
<dd><p>Flatten a <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> with a new shape. The transformation is not made in-place.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="nets.tensor.Tensor.item">
<code class="sig-name descname">item</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/tensor.html#Tensor.item"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.tensor.Tensor.item" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the item (float, int etc.) of a 0-dimensional <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>. It will detach the tensor from the computational
graph by setting <code class="docutils literal notranslate"><span class="pre">_hooks</span> <span class="pre">=</span> <span class="pre">[]</span></code> and <code class="docutils literal notranslate"><span class="pre">grad</span> <span class="pre">=</span> <span class="pre">None</span></code> to free memory and send this graph to the garbage collector.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Any</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="nets.tensor.Tensor.numpy">
<code class="sig-name descname">numpy</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/tensor.html#Tensor.numpy"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.tensor.Tensor.numpy" title="Permalink to this definition">¶</a></dt>
<dd><p>Convert the <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> data to a <code class="docutils literal notranslate"><span class="pre">numpy.ndarray</span></code> object.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>numpy.ndarray</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="nets.tensor.Tensor.register_hook">
<code class="sig-name descname">register_hook</code><span class="sig-paren">(</span><em class="sig-param">hook</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/tensor.html#Tensor.register_hook"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.tensor.Tensor.register_hook" title="Permalink to this definition">¶</a></dt>
<dd><p>Register a hook to a <code class="docutils literal notranslate"><span class="pre">Tensor</span></code></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>hook</strong> (<a class="reference internal" href="#nets.autograd.hook.Hook" title="nets.autograd.hook.Hook"><em>Hook</em></a>) – hook to register</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>None</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="nets.tensor.Tensor.reshape">
<code class="sig-name descname">reshape</code><span class="sig-paren">(</span><em class="sig-param">*shapes</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/tensor.html#Tensor.reshape"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.tensor.Tensor.reshape" title="Permalink to this definition">¶</a></dt>
<dd><p>Reshape a <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> with a new shape. The transformation is not made in-place.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The new shape <strong>must</strong> have the same size of the actual shape.
If its not the case, the reshape method will raise an error.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>int</strong> (<em>*shapes</em>) – permutation</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="nets.tensor.Tensor.sum">
<code class="sig-name descname">sum</code><span class="sig-paren">(</span><em class="sig-param">axis=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/tensor.html#Tensor.sum"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.tensor.Tensor.sum" title="Permalink to this definition">¶</a></dt>
<dd><p>Sum the data along a given axis. If no axis are specified, all values within the <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> will be summed.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>axis</strong> (<em>int</em>) – the index of the axis to sum on.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="nets.tensor.Tensor.tolist">
<code class="sig-name descname">tolist</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/tensor.html#Tensor.tolist"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.tensor.Tensor.tolist" title="Permalink to this definition">¶</a></dt>
<dd><p>Convert the <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> data to a list (of list eventually).</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>list</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="nets.tensor.Tensor.transpose">
<code class="sig-name descname">transpose</code><span class="sig-paren">(</span><em class="sig-param">*indices</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/tensor.html#Tensor.transpose"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.tensor.Tensor.transpose" title="Permalink to this definition">¶</a></dt>
<dd><p>Transpose the <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>. The operation is not in-place.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>indices</strong> (<em>tuple</em>) – permutation</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="nets.tensor.Tensor.zero_grad">
<code class="sig-name descname">zero_grad</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/tensor.html#Tensor.zero_grad"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.tensor.Tensor.zero_grad" title="Permalink to this definition">¶</a></dt>
<dd><p>Set to a zero <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> the gradient. This is call when initializing a <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> that requires gradient
tracking, or re-initialize parameters’s gradient after a training loop as they accumulate on each other.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>None</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="function">
<dt id="nets.tensor.to_array">
<code class="sig-prename descclassname">nets.tensor.</code><code class="sig-name descname">to_array</code><span class="sig-paren">(</span><em class="sig-param">arrayable</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/tensor.html#to_array"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.tensor.to_array" title="Permalink to this definition">¶</a></dt>
<dd><p>Convert an object to a <code class="docutils literal notranslate"><span class="pre">numpy.ndarray</span></code> if possible.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>arrayable</strong> – object to convert</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>numpy.ndarray</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="nets.tensor.to_tensor">
<code class="sig-prename descclassname">nets.tensor.</code><code class="sig-name descname">to_tensor</code><span class="sig-paren">(</span><em class="sig-param">tensorable</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/tensor.html#to_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.tensor.to_tensor" title="Permalink to this definition">¶</a></dt>
<dd><p>Convert an object to a <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> if possible.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>tensorable</strong> – object to convert</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Tensor</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="module-nets.numeric">
<span id="nets-numeric"></span><h2>nets.numeric<a class="headerlink" href="#module-nets.numeric" title="Permalink to this headline">¶</a></h2>
<p>The <code class="docutils literal notranslate"><span class="pre">functional</span></code> modules defines basic functions to generate tensors and transform data.</p>
</div>
<div class="section" id="module-nets.functional">
<span id="nets-functional"></span><h2>nets.functional<a class="headerlink" href="#module-nets.functional" title="Permalink to this headline">¶</a></h2>
</div>
</div>
<div class="section" id="nets-autograd">
<h1>nets.autograd<a class="headerlink" href="#nets-autograd" title="Permalink to this headline">¶</a></h1>
<div class="section" id="module-nets.autograd.functions">
<span id="nets-autograd-functions"></span><h2>nets.autograd.functions<a class="headerlink" href="#module-nets.autograd.functions" title="Permalink to this headline">¶</a></h2>
<p>This modules defines more complex operations, and defines the most popular functions such as:</p>
<ul class="simple">
<li><p>exp,</p></li>
<li><p>log,</p></li>
<li><p>tanh,</p></li>
<li><p>sigmoid;</p></li>
<li><p>relu…</p></li>
</ul>
<dl class="function">
<dt id="nets.autograd.functions.exp">
<code class="sig-prename descclassname">nets.autograd.functions.</code><code class="sig-name descname">exp</code><span class="sig-paren">(</span><em class="sig-param">t</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/autograd/functions.html#exp"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.autograd.functions.exp" title="Permalink to this definition">¶</a></dt>
<dd><p>Exponentiation f a tensor.</p>
<div class="math notranslate nohighlight">
\[T_{out} = \text{exp}(T)\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>t</strong> (<a class="reference internal" href="#nets.tensor.Tensor" title="nets.tensor.Tensor"><em>Tensor</em></a>) – input tensor to transform</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="nets.autograd.functions.leaky_relu">
<code class="sig-prename descclassname">nets.autograd.functions.</code><code class="sig-name descname">leaky_relu</code><span class="sig-paren">(</span><em class="sig-param">x: Array</em>, <em class="sig-param">alpha: Optional[float] = 0.01</em><span class="sig-paren">)</span> &#x2192; Array<a class="reference internal" href="../_modules/nets/autograd/functions.html#leaky_relu"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.autograd.functions.leaky_relu" title="Permalink to this definition">¶</a></dt>
<dd><p>Variation from the original <code class="docutils literal notranslate"><span class="pre">relu</span></code> function, which can prevent ‘dying’ <code class="docutils literal notranslate"><span class="pre">relu</span></code> thanks to a slope <code class="docutils literal notranslate"><span class="pre">alpha</span></code>.</p>
<div class="math notranslate nohighlight">
\[\text{leaky_relu(x)} = \max{(\alpha \times x, x)}\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>alpha</strong> (<em>float</em><em>, </em><em>optional</em>) – slope towards <span class="math notranslate nohighlight">\(-\infty\)</span>.</p>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>input: x (numpy.array): input to compute the <code class="docutils literal notranslate"><span class="pre">leaky_relu</span></code> function on.</p></li>
<li><p>output: y (numpy.array): <code class="docutils literal notranslate"><span class="pre">leaky</span> <span class="pre">relu</span></code> output, with the same shape than <code class="docutils literal notranslate"><span class="pre">x</span></code>.</p></li>
</ul>
</dd>
</dl>
<img alt="../_images/functional_leaky_relu.png" src="../_images/functional_leaky_relu.png" />
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">in_array</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out_array</span> <span class="o">=</span> <span class="n">leaky_relu</span><span class="p">(</span><span class="n">in_array</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span>
</pre></div>
</div>
<p>See <a class="reference internal" href="#nets.nn.activation.LeakyReLU" title="nets.nn.activation.LeakyReLU"><code class="xref py py-class docutils literal notranslate"><span class="pre">LeakyReLU</span></code></a> for the activation implementation.</p>
</dd></dl>

<dl class="function">
<dt id="nets.autograd.functions.leaky_relu_prime">
<code class="sig-prename descclassname">nets.autograd.functions.</code><code class="sig-name descname">leaky_relu_prime</code><span class="sig-paren">(</span><em class="sig-param">x: Array</em>, <em class="sig-param">alpha: Optional[float] = 0.01</em><span class="sig-paren">)</span> &#x2192; Array<a class="reference internal" href="../_modules/nets/autograd/functions.html#leaky_relu_prime"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.autograd.functions.leaky_relu_prime" title="Permalink to this definition">¶</a></dt>
<dd><p>First order derivative of <code class="docutils literal notranslate"><span class="pre">leaky_relu</span></code> function, defined as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\text{leaky_relu'(x)} =
                \begin{cases}
                  1, &amp;\quad x \ge 0 \\
                  \alpha, &amp;\quad x &lt; 0.
                \end{cases}\end{split}\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>alpha</strong> (<em>float</em><em>, </em><em>optional</em>) – slope towards <span class="math notranslate nohighlight">\(-\infty\)</span>.</p>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>input: x (numpy.array): input to compute the <code class="docutils literal notranslate"><span class="pre">leaky_relu_prime</span></code> function on.</p></li>
<li><p>output: y (numpy.array): derivative of the input, with the same shape than <code class="docutils literal notranslate"><span class="pre">x</span></code>.</p></li>
</ul>
</dd>
</dl>
<img alt="../_images/functional_leaky_relu_prime.png" src="../_images/functional_leaky_relu_prime.png" />
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">in_array</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out_array</span> <span class="o">=</span> <span class="n">leaky_relu_prime</span><span class="p">(</span><span class="n">in_array</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span>
</pre></div>
</div>
<p>See <a class="reference internal" href="#nets.nn.activation.LeakyReLU" title="nets.nn.activation.LeakyReLU"><code class="xref py py-class docutils literal notranslate"><span class="pre">LeakyReLU</span></code></a> for the activation implementation.</p>
</dd></dl>

<dl class="function">
<dt id="nets.autograd.functions.log">
<code class="sig-prename descclassname">nets.autograd.functions.</code><code class="sig-name descname">log</code><span class="sig-paren">(</span><em class="sig-param">t</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/autograd/functions.html#log"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.autograd.functions.log" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the logarithm of a tensor object.</p>
<div class="math notranslate nohighlight">
\[T_{out} = \text{log}(T)\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>t</strong> (<em>Tensor like</em>) – input tensor</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="nets.autograd.functions.maximum">
<code class="sig-prename descclassname">nets.autograd.functions.</code><code class="sig-name descname">maximum</code><span class="sig-paren">(</span><em class="sig-param">t1</em>, <em class="sig-param">t2</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/autograd/functions.html#maximum"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.autograd.functions.maximum" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the maximum between two tensors.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>t1</strong> (<a class="reference internal" href="#nets.tensor.Tensor" title="nets.tensor.Tensor"><em>Tensor</em></a>) – input tensor to merge</p></li>
<li><p><strong>t2</strong> (<a class="reference internal" href="#nets.tensor.Tensor" title="nets.tensor.Tensor"><em>Tensor</em></a>) – input tensor to merge</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="nets.autograd.functions.minimum">
<code class="sig-prename descclassname">nets.autograd.functions.</code><code class="sig-name descname">minimum</code><span class="sig-paren">(</span><em class="sig-param">t1</em>, <em class="sig-param">t2</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/autograd/functions.html#minimum"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.autograd.functions.minimum" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the minimum between two tensors.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>t1</strong> (<a class="reference internal" href="#nets.tensor.Tensor" title="nets.tensor.Tensor"><em>Tensor</em></a>) – input tensor to merge</p></li>
<li><p><strong>t2</strong> (<a class="reference internal" href="#nets.tensor.Tensor" title="nets.tensor.Tensor"><em>Tensor</em></a>) – input tensor to merge</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="nets.autograd.functions.pow">
<code class="sig-prename descclassname">nets.autograd.functions.</code><code class="sig-name descname">pow</code><span class="sig-paren">(</span><em class="sig-param">t</em>, <em class="sig-param">power</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/autograd/functions.html#pow"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.autograd.functions.pow" title="Permalink to this definition">¶</a></dt>
<dd><p>Power a tensor-like object.</p>
<div class="math notranslate nohighlight">
\[T_{out} = T^2\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>t</strong> (<em>Tensor like</em>) – reference tensor</p></li>
<li><p><strong>power</strong> (<em>int</em>) – power to elevate a tensor</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="nets.autograd.functions.relu">
<code class="sig-prename descclassname">nets.autograd.functions.</code><code class="sig-name descname">relu</code><span class="sig-paren">(</span><em class="sig-param">x: Array</em><span class="sig-paren">)</span> &#x2192; Array<a class="reference internal" href="../_modules/nets/autograd/functions.html#relu"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.autograd.functions.relu" title="Permalink to this definition">¶</a></dt>
<dd><p><code class="docutils literal notranslate"><span class="pre">relu</span></code> is a standard activation function, defined as:</p>
<div class="math notranslate nohighlight">
\[\text{relu(x)} = \max{(0, x)}\]</div>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>input: x (numpy.array): input to compute the <code class="docutils literal notranslate"><span class="pre">relu</span></code> function on.</p></li>
<li><p>output: y (numpy.array): <code class="docutils literal notranslate"><span class="pre">relu</span></code> output, with the same shape than <code class="docutils literal notranslate"><span class="pre">x</span></code>.</p></li>
</ul>
</dd>
</dl>
<img alt="../_images/functional_relu.png" src="../_images/functional_relu.png" />
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">in_array</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out_array</span> <span class="o">=</span> <span class="n">relu</span><span class="p">(</span><span class="n">in_array</span><span class="p">)</span>
</pre></div>
</div>
<p>See <a class="reference internal" href="#nets.nn.activation.ReLU" title="nets.nn.activation.ReLU"><code class="xref py py-class docutils literal notranslate"><span class="pre">ReLU</span></code></a> for the activation implementation.</p>
</dd></dl>

<dl class="function">
<dt id="nets.autograd.functions.relu_prime">
<code class="sig-prename descclassname">nets.autograd.functions.</code><code class="sig-name descname">relu_prime</code><span class="sig-paren">(</span><em class="sig-param">x: Array</em><span class="sig-paren">)</span> &#x2192; Array<a class="reference internal" href="../_modules/nets/autograd/functions.html#relu_prime"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.autograd.functions.relu_prime" title="Permalink to this definition">¶</a></dt>
<dd><p>First order derivative of the <code class="docutils literal notranslate"><span class="pre">relu</span></code> function.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\text{relu'(x)} =
                \begin{cases}
                  1, &amp;\quad x \ge 0 \\
                  0, &amp;\quad x &lt; 0.
                \end{cases}\end{split}\]</div>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>input: x (numpy.array): input to compute the <code class="docutils literal notranslate"><span class="pre">leaky_relu</span></code> function on.</p></li>
<li><p>output: y (numpy.array): gradient of the input, with the same shape than <code class="docutils literal notranslate"><span class="pre">x</span></code>.</p></li>
</ul>
</dd>
</dl>
<img alt="../_images/functional_relu_prime.png" src="../_images/functional_relu_prime.png" />
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">in_array</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out_array</span> <span class="o">=</span> <span class="n">relu_prime</span><span class="p">(</span><span class="n">in_array</span><span class="p">)</span>
</pre></div>
</div>
<p>See <a class="reference internal" href="#nets.nn.activation.ReLU" title="nets.nn.activation.ReLU"><code class="xref py py-class docutils literal notranslate"><span class="pre">ReLU</span></code></a> for the activation implementation.</p>
</dd></dl>

<dl class="function">
<dt id="nets.autograd.functions.sigmoid">
<code class="sig-prename descclassname">nets.autograd.functions.</code><code class="sig-name descname">sigmoid</code><span class="sig-paren">(</span><em class="sig-param">x: Array</em><span class="sig-paren">)</span> &#x2192; Array<a class="reference internal" href="../_modules/nets/autograd/functions.html#sigmoid"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.autograd.functions.sigmoid" title="Permalink to this definition">¶</a></dt>
<dd><p>Vanilla <code class="docutils literal notranslate"><span class="pre">sigmoid</span></code> function, defined as:</p>
<div class="math notranslate nohighlight">
\[\text{sigmoid}(x) = \frac{1}{1 + e^{-x}}\]</div>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>input: x (numpy.array): input to compute the <code class="docutils literal notranslate"><span class="pre">sigmoid</span></code> function on.</p></li>
<li><p>output: y (numpy.array): <code class="docutils literal notranslate"><span class="pre">sigmoid</span></code> output, with the same shape than <code class="docutils literal notranslate"><span class="pre">x</span></code>.</p></li>
</ul>
</dd>
</dl>
<img alt="../_images/functional_sigmoid.png" src="../_images/functional_sigmoid.png" />
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">in_array</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out_array</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">in_array</span><span class="p">)</span>
</pre></div>
</div>
<p>See <a class="reference internal" href="#nets.nn.activation.Sigmoid" title="nets.nn.activation.Sigmoid"><code class="xref py py-class docutils literal notranslate"><span class="pre">Sigmoid</span></code></a> for the activation implementation.</p>
</dd></dl>

<dl class="function">
<dt id="nets.autograd.functions.sigmoid_prime">
<code class="sig-prename descclassname">nets.autograd.functions.</code><code class="sig-name descname">sigmoid_prime</code><span class="sig-paren">(</span><em class="sig-param">x: Array</em><span class="sig-paren">)</span> &#x2192; Array<a class="reference internal" href="../_modules/nets/autograd/functions.html#sigmoid_prime"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.autograd.functions.sigmoid_prime" title="Permalink to this definition">¶</a></dt>
<dd><p>First order derivative of <code class="docutils literal notranslate"><span class="pre">sigmoid</span></code> function, defined as:</p>
<div class="math notranslate nohighlight">
\[\text{sigmoid'}(x) = (1 - \text{sigmoid}(x))\]</div>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>input: x (numpy.array): input to compute the <code class="docutils literal notranslate"><span class="pre">sigmoid</span> <span class="pre">derivative</span></code> function on.</p></li>
<li><p>output: y (numpy.array): gradient of the input, with the same shape than <code class="docutils literal notranslate"><span class="pre">x</span></code>.</p></li>
</ul>
</dd>
</dl>
<img alt="../_images/functional_sigmoid_prime.png" src="../_images/functional_sigmoid_prime.png" />
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">in_array</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out_array</span> <span class="o">=</span> <span class="n">sigmoid_prime</span><span class="p">(</span><span class="n">in_array</span><span class="p">)</span>
</pre></div>
</div>
<p>See <a class="reference internal" href="#nets.nn.activation.Sigmoid" title="nets.nn.activation.Sigmoid"><code class="xref py py-class docutils literal notranslate"><span class="pre">Sigmoid</span></code></a> for the activation implementation.</p>
</dd></dl>

<dl class="function">
<dt id="nets.autograd.functions.softmax">
<code class="sig-prename descclassname">nets.autograd.functions.</code><code class="sig-name descname">softmax</code><span class="sig-paren">(</span><em class="sig-param">x: Array</em>, <em class="sig-param">axis: Optional[int] = 0</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="reference internal" href="../_modules/nets/autograd/functions.html#softmax"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.autograd.functions.softmax" title="Permalink to this definition">¶</a></dt>
<dd><p><code class="docutils literal notranslate"><span class="pre">softmax</span></code> function. The implementation chosen is not straight forward to prevent numerical instability.</p>
<p>The <span class="math notranslate nohighlight">\(i^{th}\)</span> element of a softmax function evaluated on a vector <span class="math notranslate nohighlight">\(x \in \mathbb{R}^n\)</span> is given by:</p>
<div class="math notranslate nohighlight">
\[\text{softmax}(x_{i}) = \frac{e^{x_i}}{\sum_{j=1}^{n} e^{x_j}}\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>axis</strong> (<em>int</em>) – axis to consider for the <code class="docutils literal notranslate"><span class="pre">softmax</span></code>. Default is <code class="docutils literal notranslate"><span class="pre">0</span></code>.</p>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>input: x (numpy.array): input to compute the <code class="docutils literal notranslate"><span class="pre">softmax</span></code> function on.</p></li>
<li><p>output: y (numpy.array): hyper-plane of the input.</p></li>
</ul>
</dd>
</dl>
<p>See <a class="reference internal" href="#nets.nn.activation.Softmax" title="nets.nn.activation.Softmax"><code class="xref py py-class docutils literal notranslate"><span class="pre">Softmax</span></code></a> for the activation implementation.</p>
</dd></dl>

<dl class="function">
<dt id="nets.autograd.functions.sqrt">
<code class="sig-prename descclassname">nets.autograd.functions.</code><code class="sig-name descname">sqrt</code><span class="sig-paren">(</span><em class="sig-param">t</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/autograd/functions.html#sqrt"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.autograd.functions.sqrt" title="Permalink to this definition">¶</a></dt>
<dd><p>Square root of a tensor-like object.</p>
<div class="math notranslate nohighlight">
\[T_{out} = T^2\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>t</strong> (<em>Tensor like</em>) – reference tensor</p></li>
<li><p><strong>power</strong> (<em>int</em>) – power to elevate a tensor</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="nets.autograd.functions.tanh">
<code class="sig-prename descclassname">nets.autograd.functions.</code><code class="sig-name descname">tanh</code><span class="sig-paren">(</span><em class="sig-param">t: Tensor</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="reference internal" href="../_modules/nets/autograd/functions.html#tanh"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.autograd.functions.tanh" title="Permalink to this definition">¶</a></dt>
<dd><p><code class="docutils literal notranslate"><span class="pre">tanh</span></code> standard function, definead as:</p>
<div class="math notranslate nohighlight">
\[\text{tanh}(T) = \frac{e^{T} - e^{-T}}{e^{T} + e^{-T}}\]</div>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>input: x (numpy.array): input to compute the <code class="docutils literal notranslate"><span class="pre">tanh</span></code> function on.</p></li>
<li><p>output: y (numpy.array): <code class="docutils literal notranslate"><span class="pre">tanh</span></code> output, with the same shape than <span class="math notranslate nohighlight">\(T\)</span>.</p></li>
</ul>
</dd>
</dl>
<img alt="../_images/functional_tanh.png" src="../_images/functional_tanh.png" />
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">nets</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">in_array</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out_array</span> <span class="o">=</span> <span class="n">nets</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">in_array</span><span class="p">)</span>
</pre></div>
</div>
<p>See <a class="reference internal" href="#nets.nn.activation.Tanh" title="nets.nn.activation.Tanh"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tanh</span></code></a> for the activation implementation.</p>
</dd></dl>

<dl class="function">
<dt id="nets.autograd.functions.tanh_prime">
<code class="sig-prename descclassname">nets.autograd.functions.</code><code class="sig-name descname">tanh_prime</code><span class="sig-paren">(</span><em class="sig-param">x: Array</em><span class="sig-paren">)</span> &#x2192; Array<a class="reference internal" href="../_modules/nets/autograd/functions.html#tanh_prime"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.autograd.functions.tanh_prime" title="Permalink to this definition">¶</a></dt>
<dd><p>First order derivative of <code class="docutils literal notranslate"><span class="pre">tanh</span></code> function, defined as:</p>
<div class="math notranslate nohighlight">
\[\text{tanh'}(x) = 1 - \text{tanh}^2(x)\]</div>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>input: x (numpy.array): input to compute the <code class="docutils literal notranslate"><span class="pre">tanh</span> <span class="pre">derivative</span></code> function on.</p></li>
<li><p>output: y (numpy.array): gradient of the input, with the same shape than <code class="docutils literal notranslate"><span class="pre">x</span></code>.</p></li>
</ul>
</dd>
</dl>
<img alt="../_images/functional_tanh_prime.png" src="../_images/functional_tanh_prime.png" />
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">in_array</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out_array</span> <span class="o">=</span> <span class="n">tanh</span><span class="p">(</span><span class="n">in_array</span><span class="p">)</span>
</pre></div>
</div>
<p>See <a class="reference internal" href="#nets.nn.activation.Tanh" title="nets.nn.activation.Tanh"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tanh</span></code></a> for the activation implementation.</p>
</dd></dl>

<dl class="function">
<dt id="nets.autograd.functions.where">
<code class="sig-prename descclassname">nets.autograd.functions.</code><code class="sig-name descname">where</code><span class="sig-paren">(</span><em class="sig-param">cond</em>, <em class="sig-param">t1</em>, <em class="sig-param">t2</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/autograd/functions.html#where"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.autograd.functions.where" title="Permalink to this definition">¶</a></dt>
<dd><p>Transformation regarding a condition.</p>
<div class="math notranslate nohighlight">
\[\begin{split}T_{out} =
                \begin{cases}
                  T_1, &amp;\quad if \quad condition \\
                  T_2, &amp;\quad else.
                \end{cases}\end{split}\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>cond</strong> (<em>bool</em>) – condition to merge two tensors</p></li>
<li><p><strong>t1</strong> (<a class="reference internal" href="#nets.tensor.Tensor" title="nets.tensor.Tensor"><em>Tensor</em></a>) – input tensor to merge</p></li>
<li><p><strong>t2</strong> (<a class="reference internal" href="#nets.tensor.Tensor" title="nets.tensor.Tensor"><em>Tensor</em></a>) – input tensor to merge</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Tensor</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="module-nets.autograd.hook">
<span id="nets-autograd-hook"></span><h2>nets.autograd.hook<a class="headerlink" href="#module-nets.autograd.hook" title="Permalink to this headline">¶</a></h2>
<p>A <code class="docutils literal notranslate"><span class="pre">Hook</span></code> keeps track of gradients and operations.</p>
<dl class="class">
<dt id="nets.autograd.hook.Hook">
<em class="property">class </em><code class="sig-prename descclassname">nets.autograd.hook.</code><code class="sig-name descname">Hook</code><span class="sig-paren">(</span><em class="sig-param">tensor</em>, <em class="sig-param">grad_fn</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/autograd/hook.html#Hook"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.autograd.hook.Hook" title="Permalink to this definition">¶</a></dt>
<dd><p>A hook is a collection of tensors and gradients.</p>
</dd></dl>

</div>
<div class="section" id="module-nets.autograd.numeric">
<span id="nets-autograd-numeric"></span><h2>nets.autograd.numeric<a class="headerlink" href="#module-nets.autograd.numeric" title="Permalink to this headline">¶</a></h2>
<p>This modules defines basic transformations on a <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> like <code class="docutils literal notranslate"><span class="pre">transpose</span></code> or <code class="docutils literal notranslate"><span class="pre">reshape</span></code>.</p>
<dl class="function">
<dt id="nets.autograd.numeric.append">
<code class="sig-prename descclassname">nets.autograd.numeric.</code><code class="sig-name descname">append</code><span class="sig-paren">(</span><em class="sig-param">t</em>, <em class="sig-param">value</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/autograd/numeric.html#append"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.autograd.numeric.append" title="Permalink to this definition">¶</a></dt>
<dd><p>Append multiples <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> from an iterable.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> in <code class="docutils literal notranslate"><span class="pre">iterable</span></code> should and must have the same shape.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>t</strong> (<a class="reference internal" href="#nets.tensor.Tensor" title="nets.tensor.Tensor"><em>Tensor</em></a>) – list containing <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> to concatenate.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the concatenation of all <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="#nets.tensor.Tensor" title="nets.tensor.Tensor">Tensor</a></p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="nets.autograd.numeric.argmax">
<code class="sig-prename descclassname">nets.autograd.numeric.</code><code class="sig-name descname">argmax</code><span class="sig-paren">(</span><em class="sig-param">t</em>, <em class="sig-param">axis=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/autograd/numeric.html#argmax"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.autograd.numeric.argmax" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the indices of maximum elements from a <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>t</strong> (<a class="reference internal" href="#nets.tensor.Tensor" title="nets.tensor.Tensor"><em>Tensor</em></a>) – tensor get maximum indices from</p></li>
<li><p><strong>axis</strong> (<em>int</em><em>, </em><em>optional</em>) – index of the axis. Default is <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="nets.autograd.numeric.concatenate">
<code class="sig-prename descclassname">nets.autograd.numeric.</code><code class="sig-name descname">concatenate</code><span class="sig-paren">(</span><em class="sig-param">iterable</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/autograd/numeric.html#concatenate"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.autograd.numeric.concatenate" title="Permalink to this definition">¶</a></dt>
<dd><p>Concatenate multiples <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> from an iterable.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> in <code class="docutils literal notranslate"><span class="pre">iterable</span></code> should and must have the same shape.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>iterable</strong> (<em>tuple</em><em>, </em><em>list</em>) – list containing <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> to concatenate.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the concatenation of all <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="#nets.tensor.Tensor" title="nets.tensor.Tensor">Tensor</a></p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="nets.autograd.numeric.flatten">
<code class="sig-prename descclassname">nets.autograd.numeric.</code><code class="sig-name descname">flatten</code><span class="sig-paren">(</span><em class="sig-param">t</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/autograd/numeric.html#flatten"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.autograd.numeric.flatten" title="Permalink to this definition">¶</a></dt>
<dd><p>Reshape in 1-dimensional a <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>t</strong> (<a class="reference internal" href="#nets.tensor.Tensor" title="nets.tensor.Tensor"><em>Tensor</em></a>) – tensor get reshape.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="nets.autograd.numeric.max">
<code class="sig-prename descclassname">nets.autograd.numeric.</code><code class="sig-name descname">max</code><span class="sig-paren">(</span><em class="sig-param">t</em>, <em class="sig-param">axis=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/autograd/numeric.html#max"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.autograd.numeric.max" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the maximum from a <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>t</strong> (<a class="reference internal" href="#nets.tensor.Tensor" title="nets.tensor.Tensor"><em>Tensor</em></a>) – tensor to transform</p></li>
<li><p><strong>axis</strong> (<em>int</em><em>, </em><em>optional</em>) – index of the axis to search. Default is <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="nets.autograd.numeric.pad">
<code class="sig-prename descclassname">nets.autograd.numeric.</code><code class="sig-name descname">pad</code><span class="sig-paren">(</span><em class="sig-param">t</em>, <em class="sig-param">padding</em>, <em class="sig-param">constant_values=0</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/autograd/numeric.html#pad"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.autograd.numeric.pad" title="Permalink to this definition">¶</a></dt>
<dd><p>Reshape a <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> to a bigger size and add a <code class="docutils literal notranslate"><span class="pre">padding</span></code> on the side, with a <code class="docutils literal notranslate"><span class="pre">0</span></code> constant value.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>t</strong> (<a class="reference internal" href="#nets.tensor.Tensor" title="nets.tensor.Tensor"><em>Tensor</em></a>) – tensor to transform</p></li>
<li><p><strong>padding</strong> (<em>tuple</em>) – padding dimensions</p></li>
<li><p><strong>constant_values</strong> (<em>scalar</em><em>, </em><em>optional</em>) – scalar affected in the padding</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="nets.autograd.numeric.reshape">
<code class="sig-prename descclassname">nets.autograd.numeric.</code><code class="sig-name descname">reshape</code><span class="sig-paren">(</span><em class="sig-param">t</em>, <em class="sig-param">shape</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/autograd/numeric.html#reshape"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.autograd.numeric.reshape" title="Permalink to this definition">¶</a></dt>
<dd><p>Reshape a <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>t</strong> (<a class="reference internal" href="#nets.tensor.Tensor" title="nets.tensor.Tensor"><em>Tensor</em></a>) – tensor to transform</p></li>
<li><p><strong>shape</strong> (<em>tuple</em>) – new shape of <code class="docutils literal notranslate"><span class="pre">t</span></code></p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="nets.autograd.numeric.transpose">
<code class="sig-prename descclassname">nets.autograd.numeric.</code><code class="sig-name descname">transpose</code><span class="sig-paren">(</span><em class="sig-param">t</em>, <em class="sig-param">indices=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/autograd/numeric.html#transpose"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.autograd.numeric.transpose" title="Permalink to this definition">¶</a></dt>
<dd><p>Permutation a tensor object.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>t</strong> (<a class="reference internal" href="#nets.tensor.Tensor" title="nets.tensor.Tensor"><em>Tensor</em></a>) – </p></li>
<li><p><strong>indices</strong> (<em>tuple</em><em>, </em><em>optional</em>) – index to transpose.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Tensor</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="module-nets.autograd.ops">
<span id="nets-autograd-ops"></span><h2>nets.autograd.ops<a class="headerlink" href="#module-nets.autograd.ops" title="Permalink to this headline">¶</a></h2>
<p>Defines basic operations between two tensors, like addition, subtraction, dot product etc.</p>
<dl class="function">
<dt id="nets.autograd.ops.add">
<code class="sig-prename descclassname">nets.autograd.ops.</code><code class="sig-name descname">add</code><span class="sig-paren">(</span><em class="sig-param">t1</em>, <em class="sig-param">t2</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/autograd/ops.html#add"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.autograd.ops.add" title="Permalink to this definition">¶</a></dt>
<dd><p>Add two tensor-like together.</p>
<div class="math notranslate nohighlight">
\[T_{out} = T_1 + T_2\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>t1</strong> (<em>Tensor like</em>) – tensor to add</p></li>
<li><p><strong>t2</strong> (<em>Tensor like</em>) – second tensor to add with</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the sum of two Tensor-like object</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="#nets.tensor.Tensor" title="nets.tensor.Tensor">Tensor</a></p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="nets.autograd.ops.div">
<code class="sig-prename descclassname">nets.autograd.ops.</code><code class="sig-name descname">div</code><span class="sig-paren">(</span><em class="sig-param">t1</em>, <em class="sig-param">t2</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/autograd/ops.html#div"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.autograd.ops.div" title="Permalink to this definition">¶</a></dt>
<dd><p>Divide two tensor-like object.</p>
<div class="math notranslate nohighlight">
\[T_{out} = T_1 \times \frac{1}{T_2}\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>t1</strong> (<em>Tensor like</em>) – tensor to multiply</p></li>
<li><p><strong>t2</strong> (<em>Tensor like</em>) – tensor to invert</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="nets.autograd.ops.dot">
<code class="sig-prename descclassname">nets.autograd.ops.</code><code class="sig-name descname">dot</code><span class="sig-paren">(</span><em class="sig-param">t1</em>, <em class="sig-param">t2</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/autograd/ops.html#dot"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.autograd.ops.dot" title="Permalink to this definition">¶</a></dt>
<dd><p>Dot product of two matrices.</p>
<div class="math notranslate nohighlight">
\[T_{out} = (t_{i, j}^{[out]})_{i, j} \quad where \quad t_{i, j}^{[out]} = \sum_{k=1}^{n} t_{i, k}^{[1]} \times
t_{k, j}^{[2]}\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>t1</strong> (<em>Tensor like</em>) – </p></li>
<li><p><strong>t2</strong> (<em>Tensor like</em>) – </p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="nets.autograd.ops.eq">
<code class="sig-prename descclassname">nets.autograd.ops.</code><code class="sig-name descname">eq</code><span class="sig-paren">(</span><em class="sig-param">t</em>, <em class="sig-param">other</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/autograd/ops.html#eq"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.autograd.ops.eq" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a boolean tensor for <em>equal</em> condition.</p>
<div class="math notranslate nohighlight">
\[condition = T == other\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>t</strong> (<a class="reference internal" href="#nets.tensor.Tensor" title="nets.tensor.Tensor"><em>Tensor</em></a>) – tensor to compare</p></li>
<li><p><strong>other</strong> (<em>Tensor like</em>) – object to compare the tensor</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="nets.autograd.ops.ge">
<code class="sig-prename descclassname">nets.autograd.ops.</code><code class="sig-name descname">ge</code><span class="sig-paren">(</span><em class="sig-param">t</em>, <em class="sig-param">other</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/autograd/ops.html#ge"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.autograd.ops.ge" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a boolean tensor for <em>greater or equal</em> condition.</p>
<div class="math notranslate nohighlight">
\[condition = T \ge other\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>t</strong> (<a class="reference internal" href="#nets.tensor.Tensor" title="nets.tensor.Tensor"><em>Tensor</em></a>) – tensor to compare</p></li>
<li><p><strong>other</strong> (<em>Tensor like</em>) – object to compare the tensor</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="nets.autograd.ops.gt">
<code class="sig-prename descclassname">nets.autograd.ops.</code><code class="sig-name descname">gt</code><span class="sig-paren">(</span><em class="sig-param">t</em>, <em class="sig-param">other</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/autograd/ops.html#gt"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.autograd.ops.gt" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a boolean tensor for <em>greater than</em> condition.</p>
<div class="math notranslate nohighlight">
\[condition = T &gt; other\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>t</strong> (<a class="reference internal" href="#nets.tensor.Tensor" title="nets.tensor.Tensor"><em>Tensor</em></a>) – tensor to compare</p></li>
<li><p><strong>other</strong> (<em>Tensor like</em>) – object to compare the tensor</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="nets.autograd.ops.inverse">
<code class="sig-prename descclassname">nets.autograd.ops.</code><code class="sig-name descname">inverse</code><span class="sig-paren">(</span><em class="sig-param">t</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/autograd/ops.html#inverse"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.autograd.ops.inverse" title="Permalink to this definition">¶</a></dt>
<dd><p>Inverse a tensor-like object.</p>
<div class="math notranslate nohighlight">
\[T_{out} = \frac{1}{T}\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>t</strong> (<em>Tensor like</em>) – tensor to inverse.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="nets.autograd.ops.le">
<code class="sig-prename descclassname">nets.autograd.ops.</code><code class="sig-name descname">le</code><span class="sig-paren">(</span><em class="sig-param">t</em>, <em class="sig-param">other</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/autograd/ops.html#le"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.autograd.ops.le" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a boolean tensor for <em>lower or equal</em> condition.</p>
<div class="math notranslate nohighlight">
\[condition = T \le other\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>t</strong> (<a class="reference internal" href="#nets.tensor.Tensor" title="nets.tensor.Tensor"><em>Tensor</em></a>) – tensor to compare</p></li>
<li><p><strong>other</strong> (<em>Tensor like</em>) – object to compare the tensor</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="nets.autograd.ops.lt">
<code class="sig-prename descclassname">nets.autograd.ops.</code><code class="sig-name descname">lt</code><span class="sig-paren">(</span><em class="sig-param">t</em>, <em class="sig-param">other</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/autograd/ops.html#lt"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.autograd.ops.lt" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a boolean tensor for <em>lower than</em> condition.</p>
<div class="math notranslate nohighlight">
\[condition = T &lt; other\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>t</strong> (<a class="reference internal" href="#nets.tensor.Tensor" title="nets.tensor.Tensor"><em>Tensor</em></a>) – tensor to compare</p></li>
<li><p><strong>other</strong> (<em>Tensor like</em>) – object to compare the tensor</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="nets.autograd.ops.multiply">
<code class="sig-prename descclassname">nets.autograd.ops.</code><code class="sig-name descname">multiply</code><span class="sig-paren">(</span><em class="sig-param">t1</em>, <em class="sig-param">t2</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/autograd/ops.html#multiply"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.autograd.ops.multiply" title="Permalink to this definition">¶</a></dt>
<dd><p>Elementwise multiplication of two tensors-like object.</p>
<div class="math notranslate nohighlight">
\[T_{out} = T_1 \times T_2\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>t1</strong> (<em>Tensor like</em>) – tensor to multiply</p></li>
<li><p><strong>t2</strong> (<em>Tensor like</em>) – second tensor to multiply with</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="nets.autograd.ops.ne">
<code class="sig-prename descclassname">nets.autograd.ops.</code><code class="sig-name descname">ne</code><span class="sig-paren">(</span><em class="sig-param">t</em>, <em class="sig-param">other</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/autograd/ops.html#ne"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.autograd.ops.ne" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a boolean tensor for <em>not equal</em> condition.</p>
<div class="math notranslate nohighlight">
\[condition = T not other\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>t</strong> (<a class="reference internal" href="#nets.tensor.Tensor" title="nets.tensor.Tensor"><em>Tensor</em></a>) – tensor to compare</p></li>
<li><p><strong>other</strong> (<em>Tensor like</em>) – object to compare the tensor</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="nets.autograd.ops.neg">
<code class="sig-prename descclassname">nets.autograd.ops.</code><code class="sig-name descname">neg</code><span class="sig-paren">(</span><em class="sig-param">t</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/autograd/ops.html#neg"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.autograd.ops.neg" title="Permalink to this definition">¶</a></dt>
<dd><p>Oppose the values of a tensor.</p>
<div class="math notranslate nohighlight">
\[T_{out} = - T\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>t</strong> (<a class="reference internal" href="#nets.tensor.Tensor" title="nets.tensor.Tensor"><em>Tensor</em></a>) – tensor  to oppose.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="nets.autograd.ops.slice">
<code class="sig-prename descclassname">nets.autograd.ops.</code><code class="sig-name descname">slice</code><span class="sig-paren">(</span><em class="sig-param">t</em>, <em class="sig-param">indices</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/autograd/ops.html#slice"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.autograd.ops.slice" title="Permalink to this definition">¶</a></dt>
<dd><p>Slice a tensor from given indices.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>t</strong> (<a class="reference internal" href="#nets.tensor.Tensor" title="nets.tensor.Tensor"><em>Tensor</em></a>) – tensor to slice</p></li>
<li><p><strong>(</strong><strong>tuple</strong><strong>, </strong><strong>int</strong><strong>,</strong> (<em>idxs</em>) – ): indices to extract data</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="nets.autograd.ops.sub">
<code class="sig-prename descclassname">nets.autograd.ops.</code><code class="sig-name descname">sub</code><span class="sig-paren">(</span><em class="sig-param">t1</em>, <em class="sig-param">t2</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/autograd/ops.html#sub"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.autograd.ops.sub" title="Permalink to this definition">¶</a></dt>
<dd><p>Subtract two tensor-like object</p>
<div class="math notranslate nohighlight">
\[T_{out} = T_1 - T_2\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>t1</strong> (<em>Tensor like</em>) – tensor to subtract</p></li>
<li><p><strong>t2</strong> (<em>Tensor like</em>) – second tensor to subtract with</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="nets.autograd.ops.sum">
<code class="sig-prename descclassname">nets.autograd.ops.</code><code class="sig-name descname">sum</code><span class="sig-paren">(</span><em class="sig-param">t</em>, <em class="sig-param">axis=None</em>, <em class="sig-param">keepdims=False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/autograd/ops.html#sum"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.autograd.ops.sum" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the sum of all elements in a tensor, and update the gradients and hooks.</p>
<div class="math notranslate nohighlight">
\[\text{sum} = \sum_{idx} t_{idx}\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>t</strong> (<a class="reference internal" href="#nets.tensor.Tensor" title="nets.tensor.Tensor"><em>Tensor</em></a>) – tensor to get the sum from</p></li>
<li><p><strong>axis</strong> (<em>int</em>) – axis to sum</p></li>
<li><p><strong>keepdims</strong> (<em>bool</em>) – keep the same dimension in the resulting <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> as the input if set to <code class="docutils literal notranslate"><span class="pre">True</span></code>.
Default is <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>summed tensor</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="#nets.tensor.Tensor" title="nets.tensor.Tensor">Tensor</a></p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="module-nets.autograd.parameter">
<span id="nets-autograd-parameter"></span><h2>nets.autograd.parameter<a class="headerlink" href="#module-nets.autograd.parameter" title="Permalink to this headline">¶</a></h2>
<p>A parameter is a trainable tensor.</p>
<dl class="class">
<dt id="nets.autograd.parameter.Parameter">
<em class="property">class </em><code class="sig-prename descclassname">nets.autograd.parameter.</code><code class="sig-name descname">Parameter</code><span class="sig-paren">(</span><em class="sig-param">data=None</em>, <em class="sig-param">shape=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/autograd/parameter.html#Parameter"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.autograd.parameter.Parameter" title="Permalink to this definition">¶</a></dt>
<dd><p>Instantiate a parameter, made of trainable data. A trainable data is a value that will be updated during
the back-propagation, usually it refers to <code class="docutils literal notranslate"><span class="pre">weights</span></code> and <code class="docutils literal notranslate"><span class="pre">biases</span></code> of a layer.</p>
<dl class="method">
<dt id="nets.autograd.parameter.Parameter.normal">
<em class="property">classmethod </em><code class="sig-name descname">normal</code><span class="sig-paren">(</span><em class="sig-param">shape</em>, <em class="sig-param">mu=0</em>, <em class="sig-param">sigma=1</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/autograd/parameter.html#Parameter.normal"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.autograd.parameter.Parameter.normal" title="Permalink to this definition">¶</a></dt>
<dd><p>Generate a <code class="docutils literal notranslate"><span class="pre">Parameter</span></code> following a normal distribution center at <code class="docutils literal notranslate"><span class="pre">mu</span></code> with a standard deviation of
<code class="docutils literal notranslate"><span class="pre">sigma</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>shape</strong> (<em>tuple</em>) – shape of the <code class="docutils literal notranslate"><span class="pre">Parameter</span></code></p></li>
<li><p><strong>mu</strong> (<em>scalar</em>) – mean of the normal distribution. Default is <code class="docutils literal notranslate"><span class="pre">0</span></code>.</p></li>
<li><p><strong>sigma</strong> (<em>scalar</em>) – standard deviation of the normal distribution. Default is <code class="docutils literal notranslate"><span class="pre">1</span></code>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Parameter</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="nets.autograd.parameter.Parameter.orthogonal">
<em class="property">classmethod </em><code class="sig-name descname">orthogonal</code><span class="sig-paren">(</span><em class="sig-param">shape</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/autograd/parameter.html#Parameter.orthogonal"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.autograd.parameter.Parameter.orthogonal" title="Permalink to this definition">¶</a></dt>
<dd><p>Initializes weight parameters orthogonally.
From the [exercise 02456 from DTU course](<a class="reference external" href="https://github.com/DeepLearningDTU/02456-deep-learning-with-PyTorch">https://github.com/DeepLearningDTU/02456-deep-learning-with-PyTorch</a>).</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Refer to [this paper](<a class="reference external" href="https://arxiv.org/abs/1312.6120">https://arxiv.org/abs/1312.6120</a>) for an explanation of this initialization.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>shape</strong> (<em>tuple</em>) – shape of dimensionality greater than 2 (weight matrix)</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Parameter</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="nets.autograd.parameter.Parameter.scaled_weight">
<em class="property">classmethod </em><code class="sig-name descname">scaled_weight</code><span class="sig-paren">(</span><em class="sig-param">input_dim</em>, <em class="sig-param">output_dim</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/autograd/parameter.html#Parameter.scaled_weight"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.autograd.parameter.Parameter.scaled_weight" title="Permalink to this definition">¶</a></dt>
<dd><p>Scaled initialization from <span class="math notranslate nohighlight">\(He et al.\)</span></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_dim</strong> (<em>int</em>) – dimension of the input layer</p></li>
<li><p><strong>output_dim</strong> (<em>int</em>) – dimension of the output layer</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Parameter</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="nets.autograd.parameter.Parameter.uniform">
<em class="property">classmethod </em><code class="sig-name descname">uniform</code><span class="sig-paren">(</span><em class="sig-param">shape</em>, <em class="sig-param">low=-1</em>, <em class="sig-param">high=1</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/autograd/parameter.html#Parameter.uniform"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.autograd.parameter.Parameter.uniform" title="Permalink to this definition">¶</a></dt>
<dd><p>Generate a <code class="docutils literal notranslate"><span class="pre">Parameter</span></code> with data following a uniform distribution between <code class="docutils literal notranslate"><span class="pre">lower</span></code> and <code class="docutils literal notranslate"><span class="pre">upper</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>shape</strong> (<em>tuple</em>) – shape of the <code class="docutils literal notranslate"><span class="pre">Parameter</span></code></p></li>
<li><p><strong>low</strong> (<em>scalar</em>) – lower bound of the uniform distribution. Default is <code class="docutils literal notranslate"><span class="pre">-1</span></code>.</p></li>
<li><p><strong>high</strong> (<em>scalar</em>) – upper bound of the uniform distribution. Default is <code class="docutils literal notranslate"><span class="pre">1</span></code>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Parameter</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="nets.autograd.parameter.Parameter.zeros">
<em class="property">classmethod </em><code class="sig-name descname">zeros</code><span class="sig-paren">(</span><em class="sig-param">shape</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/autograd/parameter.html#Parameter.zeros"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.autograd.parameter.Parameter.zeros" title="Permalink to this definition">¶</a></dt>
<dd><p>Generate a zero-Parameter</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>shape</strong> (<em>tuple</em>) – shape of the <code class="docutils literal notranslate"><span class="pre">Parameter</span></code></p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Parameter</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
</div>
<div class="section" id="nets-nn">
<h1>nets.nn<a class="headerlink" href="#nets-nn" title="Permalink to this headline">¶</a></h1>
<p>The main core of <strong>NETS</strong> is located in <strong>nn</strong> package.
The neural networks implemented are the most popular ones:</p>
<ul class="simple">
<li><p><a class="reference external" href="nets.nn.linear">Linear Neural Network</a>,</p></li>
<li><p><a class="reference external" href="nets.nn.dnn">Dense Neural Network</a>,</p></li>
<li><p><a class="reference external" href="nets.nn.cnn">Convolutional Neural Network</a>,</p></li>
<li><p><a class="reference external" href="nets.nn.rnn">Recurent Neural Network</a>.</p></li>
</ul>
<div class="section" id="module-nets.nn.activation">
<span id="nets-nn-activation"></span><h2>nets.nn.activation<a class="headerlink" href="#module-nets.nn.activation" title="Permalink to this headline">¶</a></h2>
<p>This modules defines all activation function, used in neural networks to add non-linearity from one layer to another.</p>
<p>Usage:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">nets.nn.activation</span> <span class="k">as</span> <span class="nn">A</span>

<span class="c1"># Define a ReLU activation function</span>
<span class="c1"># This activation function is popular as an hidden activation function</span>
<span class="n">activation_function</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
<span class="c1"># Defines 3 1-D array of length 5, stacked together.</span>
<span class="n">batch_input</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="o">-</span><span class="mi">7</span><span class="p">],</span>
                        <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span>
                        <span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>

<span class="c1"># Check the result for one single pass</span>
<span class="n">batch_output</span> <span class="o">=</span> <span class="n">activation_function</span><span class="p">(</span><span class="n">batch_input</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">batch_output</span><span class="p">)</span>

<span class="c1"># Check the backward pass</span>
<span class="n">backward</span> <span class="o">=</span> <span class="n">ReLU</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">batch_ouput</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">backward</span><span class="p">)</span>
</pre></div>
</div>
<blockquote class="sphx-glr-script-out">
<div><p>Out:</p>
</div></blockquote>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>array([[1, 0, 5, 10, 0],
       [2, 4, 3, 0, 5],
       [0, 2, 8, 4, 0]])
array([[1, 0, 1, 1, 0],
       [1, 1, 1, 0, 1],
       [0, 1, 1, 1, 0]])
</pre></div>
</div>
<dl class="class">
<dt id="nets.nn.activation.Activation">
<em class="property">class </em><code class="sig-prename descclassname">nets.nn.activation.</code><code class="sig-name descname">Activation</code><span class="sig-paren">(</span><em class="sig-param">func</em>, <em class="sig-param">func_prime</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/nn/activation.html#Activation"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.nn.activation.Activation" title="Permalink to this definition">¶</a></dt>
<dd><p>An activation modules is a transformation that modify its inputs element wise, usually it uses non-linearity
functions.</p>
<dl class="method">
<dt id="nets.nn.activation.Activation.backward">
<code class="sig-name descname">backward</code><span class="sig-paren">(</span><em class="sig-param">grad</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/nn/activation.html#Activation.backward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.nn.activation.Activation.backward" title="Permalink to this definition">¶</a></dt>
<dd><p>One backward step.</p>
</dd></dl>

<dl class="method">
<dt id="nets.nn.activation.Activation.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">inputs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/nn/activation.html#Activation.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.nn.activation.Activation.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>One forward step. Gradients and outputs should be saved in the <code class="docutils literal notranslate"><span class="pre">_cache</span></code> when training, to be able to
perform the backward pass.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="nets.nn.activation.LeakyReLU">
<em class="property">class </em><code class="sig-prename descclassname">nets.nn.activation.</code><code class="sig-name descname">LeakyReLU</code><a class="reference internal" href="../_modules/nets/nn/activation.html#LeakyReLU"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.nn.activation.LeakyReLU" title="Permalink to this definition">¶</a></dt>
<dd><p><code class="docutils literal notranslate"><span class="pre">LeakyReLU</span></code> activation function.</p>
<p><span class="math notranslate nohighlight">\(\text{LeakyReLU.forward}(x) = \text{leaky_relu(x)} = \max{(\alpha \times x, x)}\)</span></p>
<p><span class="math notranslate nohighlight">\(\text{LeakyReLU.backward}(x) = \text{leaky_relu'(x)} = \begin{cases} 1, &amp;\quad x \ge 0 \\ \alpha, &amp;\quad x &lt; 0. \end{cases}\)</span></p>
<img alt="../_images/activation_LeakyReLU.png" src="../_images/activation_LeakyReLU.png" />
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">activation</span> <span class="o">=</span> <span class="n">LeakyReLU</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">batch_input</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span>
<span class="gp">... </span>                        <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="o">-</span><span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="o">-</span><span class="mi">3</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">activation</span><span class="p">(</span><span class="n">batch_input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gradient</span> <span class="o">=</span> <span class="n">activation</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>
</div>
<p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">leaky_relu()</span></code> and <code class="xref py py-func docutils literal notranslate"><span class="pre">leaky_relu_derivative()</span></code>
for the functional implementation.</p>
</dd></dl>

<dl class="class">
<dt id="nets.nn.activation.ReLU">
<em class="property">class </em><code class="sig-prename descclassname">nets.nn.activation.</code><code class="sig-name descname">ReLU</code><a class="reference internal" href="../_modules/nets/nn/activation.html#ReLU"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.nn.activation.ReLU" title="Permalink to this definition">¶</a></dt>
<dd><p><code class="docutils literal notranslate"><span class="pre">ReLU</span></code> activation function.</p>
<p><span class="math notranslate nohighlight">\(\text{ReLU.forward}(x) = \text{relu(x)} = \max{(0, x)}\)</span></p>
<p><span class="math notranslate nohighlight">\(\text{ReLU.backward}(x) = \text{relu'(x)} = \begin{cases} 1, &amp;\quad x \ge 0 \\ 0, &amp;\quad x &lt; 0. \end{cases}\)</span></p>
<img alt="../_images/activation_ReLU.png" src="../_images/activation_ReLU.png" />
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">activation</span> <span class="o">=</span> <span class="n">LeakyReLU</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">batch_input</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span>
<span class="gp">... </span>                        <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="o">-</span><span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="o">-</span><span class="mi">3</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">activation</span><span class="p">(</span><span class="n">batch_input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gradient</span> <span class="o">=</span> <span class="n">activation</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>
</div>
<p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">relu()</span></code> and <code class="xref py py-func docutils literal notranslate"><span class="pre">relu_derivative()</span></code>
for the functional implementation.</p>
</dd></dl>

<dl class="class">
<dt id="nets.nn.activation.Sigmoid">
<em class="property">class </em><code class="sig-prename descclassname">nets.nn.activation.</code><code class="sig-name descname">Sigmoid</code><a class="reference internal" href="../_modules/nets/nn/activation.html#Sigmoid"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.nn.activation.Sigmoid" title="Permalink to this definition">¶</a></dt>
<dd><p><code class="docutils literal notranslate"><span class="pre">Sigmoid</span></code> activation function.</p>
<p><span class="math notranslate nohighlight">\(\text{Sigmoid.forward}(x) = \text{sigmoid}(x) = \frac{1}{1 + e^{-x}}\)</span></p>
<p><span class="math notranslate nohighlight">\(\text{Sigmoid.backward}(x) = \text{sigmoid'}(x) = (1 - \text{sigmoid}(x))\)</span></p>
<img alt="../_images/activation_Sigmoid.png" src="../_images/activation_Sigmoid.png" />
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">activation</span> <span class="o">=</span> <span class="n">Sigmoid</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">batch_input</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span>
<span class="gp">... </span>                        <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="o">-</span><span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="o">-</span><span class="mi">3</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">activation</span><span class="p">(</span><span class="n">batch_input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gradient</span> <span class="o">=</span> <span class="n">activation</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>
</div>
<p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">sigmoid()</span></code> and <code class="xref py py-func docutils literal notranslate"><span class="pre">sigmoid_derivative()</span></code>
for the functional implementation.</p>
</dd></dl>

<dl class="class">
<dt id="nets.nn.activation.Softmax">
<em class="property">class </em><code class="sig-prename descclassname">nets.nn.activation.</code><code class="sig-name descname">Softmax</code><span class="sig-paren">(</span><em class="sig-param">axis=0</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/nn/activation.html#Softmax"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.nn.activation.Softmax" title="Permalink to this definition">¶</a></dt>
<dd><p>Softmax activation function, which should be used only at the last layer to normalize outputs values between
<span class="math notranslate nohighlight">\([0, 1]\)</span>.</p>
<dl class="method">
<dt id="nets.nn.activation.Softmax.backward">
<code class="sig-name descname">backward</code><span class="sig-paren">(</span><em class="sig-param">grad</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/nn/activation.html#Softmax.backward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.nn.activation.Softmax.backward" title="Permalink to this definition">¶</a></dt>
<dd><p>One backward step.</p>
</dd></dl>

<dl class="method">
<dt id="nets.nn.activation.Softmax.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">inputs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/nn/activation.html#Softmax.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.nn.activation.Softmax.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>One forward step. Gradients and outputs should be saved in the <code class="docutils literal notranslate"><span class="pre">_cache</span></code> when training, to be able to
perform the backward pass.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="nets.nn.activation.Tanh">
<em class="property">class </em><code class="sig-prename descclassname">nets.nn.activation.</code><code class="sig-name descname">Tanh</code><a class="reference internal" href="../_modules/nets/nn/activation.html#Tanh"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.nn.activation.Tanh" title="Permalink to this definition">¶</a></dt>
<dd><p><code class="docutils literal notranslate"><span class="pre">Tanh</span></code> activation function.</p>
<p><span class="math notranslate nohighlight">\(\text{Tanh.forward}(x) = \text{tanh}(x) = \frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}\)</span></p>
<p><span class="math notranslate nohighlight">\(\text{Tanh.backward}(x) = \text{tanh'}(x) = 1 - \text{tanh}^2(x)\)</span></p>
<img alt="../_images/activation_Tanh.png" src="../_images/activation_Tanh.png" />
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">activation</span> <span class="o">=</span> <span class="n">Tanh</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">batch_input</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span>
<span class="gp">... </span>                        <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="o">-</span><span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="o">-</span><span class="mi">3</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">activation</span><span class="p">(</span><span class="n">batch_input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gradient</span> <span class="o">=</span> <span class="n">activation</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>
</div>
<p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">tanh()</span></code> and <code class="xref py py-func docutils literal notranslate"><span class="pre">tanh_derivative()</span></code>
for the functional implementation.</p>
</dd></dl>

</div>
<div class="section" id="module-nets.nn.functional">
<span id="nets-nn-functional"></span><h2>nets.nn.functional<a class="headerlink" href="#module-nets.nn.functional" title="Permalink to this headline">¶</a></h2>
<p>Defines elementary functions used in Neural Network layers.</p>
<dl class="function">
<dt id="nets.nn.functional.dropout">
<code class="sig-prename descclassname">nets.nn.functional.</code><code class="sig-name descname">dropout</code><span class="sig-paren">(</span><em class="sig-param">t</em>, <em class="sig-param">prob=0.5</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/nn/functional.html#dropout"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.nn.functional.dropout" title="Permalink to this definition">¶</a></dt>
<dd><p>Zeros elements from a <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> with a probability <code class="docutils literal notranslate"><span class="pre">prob</span></code>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\text{dropout}(T) = T \times Z \quad \text{where} Z = (z_{i})_{i} \quad and z_i =
                                                                                \begin{cases}
                                                                                  1, &amp;\quad p \ge prob \\
                                                                                  0, &amp;\quad else.
                                                                                \end{cases}\end{split}\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>t</strong> (<a class="reference internal" href="#nets.tensor.Tensor" title="nets.tensor.Tensor"><em>Tensor</em></a>) – tensor to zeros</p></li>
<li><p><strong>prob</strong> (<em>float</em><em> [</em><em>0</em><em>, </em><em>1</em><em>]</em>) – probability to zero an element</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>input tensor with some zeros</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="#nets.tensor.Tensor" title="nets.tensor.Tensor">Tensor</a></p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="module-nets.nn.loss">
<span id="nets-nn-loss"></span><h2>nets.nn.loss<a class="headerlink" href="#module-nets.nn.loss" title="Permalink to this headline">¶</a></h2>
<p>Loss functions evaluate the precision and correctness of a model’s predictions. The most popular ones are <code class="docutils literal notranslate"><span class="pre">MSE</span></code>,
<code class="docutils literal notranslate"><span class="pre">CrossEntropyLoss</span></code> and <code class="docutils literal notranslate"><span class="pre">Adam</span></code>. Each loss functions presents advantages, and results may vary when choosing one from
another.</p>
<dl class="class">
<dt id="nets.nn.loss.CrossEntropyLoss">
<em class="property">class </em><code class="sig-prename descclassname">nets.nn.loss.</code><code class="sig-name descname">CrossEntropyLoss</code><a class="reference internal" href="../_modules/nets/nn/loss.html#CrossEntropyLoss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.nn.loss.CrossEntropyLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Cross Entropy Loss. First, a softmax transformation is used to map the predictions between <span class="math notranslate nohighlight">\([0, 1]\)</span>,
then the cost is computed:</p>
<div class="math notranslate nohighlight">
\[\text{CrossEntropyLoss} = - \frac{1}{N} \sum_{i=1}^{c}labels_{i}\log(pred_{i})\]</div>
<dl class="method">
<dt id="nets.nn.loss.CrossEntropyLoss.backward">
<code class="sig-name descname">backward</code><span class="sig-paren">(</span><em class="sig-param">predictions</em>, <em class="sig-param">labels</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/nn/loss.html#CrossEntropyLoss.backward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.nn.loss.CrossEntropyLoss.backward" title="Permalink to this definition">¶</a></dt>
<dd><p>One backward step.</p>
</dd></dl>

<dl class="method">
<dt id="nets.nn.loss.CrossEntropyLoss.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">predictions</em>, <em class="sig-param">labels</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/nn/loss.html#CrossEntropyLoss.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.nn.loss.CrossEntropyLoss.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the cross entropy cost function.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>predictions</strong> (<em>numpy.array</em>) – tensor of un-normalized floats with shape <span class="math notranslate nohighlight">\((N, c)\)</span>.</p></li>
<li><p><strong>labels</strong> (<em>numpy.array</em>) – tensor of integer values with shape <span class="math notranslate nohighlight">\((N)\)</span>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the cost regarding the loss function.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>cost (float)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="nets.nn.loss.Loss">
<em class="property">class </em><code class="sig-prename descclassname">nets.nn.loss.</code><code class="sig-name descname">Loss</code><a class="reference internal" href="../_modules/nets/nn/loss.html#Loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.nn.loss.Loss" title="Permalink to this definition">¶</a></dt>
<dd><p>A loss function evaluate the correctness of a set of predictions regarding gold-labels.
The predictions should be un-corrected, <em>ie</em> no transformations like <code class="docutils literal notranslate"><span class="pre">Softmax</span></code> should have been used before.
The loss function will do the transformation if necessary.
The attribute <code class="docutils literal notranslate"><span class="pre">history</span></code> keeps track of the cost when the loss function is called.</p>
<dl class="method">
<dt id="nets.nn.loss.Loss.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">predictions</em>, <em class="sig-param">labels</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/nn/loss.html#Loss.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.nn.loss.Loss.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the cross entropy cost function.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>predictions</strong> (<em>numpy.array</em>) – tensor of un-normalized floats with shape <span class="math notranslate nohighlight">\((N, c)\)</span>.</p></li>
<li><p><strong>labels</strong> (<em>numpy.array</em>) – tensor of integer values with shape <span class="math notranslate nohighlight">\((N)\)</span>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the cost regarding the loss function.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>cost (float)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="nets.nn.loss.MSELoss">
<em class="property">class </em><code class="sig-prename descclassname">nets.nn.loss.</code><code class="sig-name descname">MSELoss</code><a class="reference internal" href="../_modules/nets/nn/loss.html#MSELoss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.nn.loss.MSELoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Mean Square Error Loss, defined as:</p>
<div class="math notranslate nohighlight">
\[\text{MSE} = \frac{1}{N}\sum_{i=1}^{c}(predictions - labels)^2\]</div>
<dl class="method">
<dt id="nets.nn.loss.MSELoss.backward">
<code class="sig-name descname">backward</code><span class="sig-paren">(</span><em class="sig-param">predictions</em>, <em class="sig-param">labels</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/nn/loss.html#MSELoss.backward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.nn.loss.MSELoss.backward" title="Permalink to this definition">¶</a></dt>
<dd><p>One backward step.</p>
</dd></dl>

<dl class="method">
<dt id="nets.nn.loss.MSELoss.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">predictions</em>, <em class="sig-param">labels</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/nn/loss.html#MSELoss.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.nn.loss.MSELoss.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the cross entropy cost function.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>predictions</strong> (<em>numpy.array</em>) – tensor of un-normalized floats with shape <span class="math notranslate nohighlight">\((N, c)\)</span>.</p></li>
<li><p><strong>labels</strong> (<em>numpy.array</em>) – tensor of integer values with shape <span class="math notranslate nohighlight">\((N)\)</span>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the cost regarding the loss function.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>cost (float)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-nets.nn.utils">
<span id="nets-nn-utils"></span><h2>nets.nn.utils<a class="headerlink" href="#module-nets.nn.utils" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="nets.nn.utils.one_hot">
<code class="sig-prename descclassname">nets.nn.utils.</code><code class="sig-name descname">one_hot</code><span class="sig-paren">(</span><em class="sig-param">Y</em>, <em class="sig-param">num_classes</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/nn/utils.html#one_hot"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.nn.utils.one_hot" title="Permalink to this definition">¶</a></dt>
<dd><p>Perform one-hot encoding on input Y.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\text{Y'}_{i, j} =
            \begin{cases}
              1, &amp;\quad if \quad Y_i = 0 \\
              0, &amp;\quad else
            \end{cases}\end{split}\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>Y</strong> (<a class="reference internal" href="#nets.tensor.Tensor" title="nets.tensor.Tensor"><em>Tensor</em></a>) – 1D tensor of classes indices of length <span class="math notranslate nohighlight">\(N\)</span></p></li>
<li><p><strong>num_classes</strong> (<em>int</em>) – number of classes <span class="math notranslate nohighlight">\(c\)</span></p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>one hot encoded tensor of shape <span class="math notranslate nohighlight">\((N, c)\)</span></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="#nets.tensor.Tensor" title="nets.tensor.Tensor">Tensor</a></p>
</dd>
</dl>
</dd></dl>

</div>
</div>
<div class="section" id="nets-nn-modules">
<h1>nets.nn.modules<a class="headerlink" href="#nets-nn-modules" title="Permalink to this headline">¶</a></h1>
<div class="section" id="module-nets.nn.modules.conv">
<span id="nets-nn-conv"></span><h2>nets.nn.conv<a class="headerlink" href="#module-nets.nn.modules.conv" title="Permalink to this headline">¶</a></h2>
<p>This modules defines a Convolution Neural Network (CNN) naively. This CNN is for test and comparisons purposes.
If you wan to use a more appropriate CNN for your models, use the <code class="docutils literal notranslate"><span class="pre">CNN</span></code> instead.</p>
<dl class="class">
<dt id="nets.nn.modules.conv.Conv2d">
<em class="property">class </em><code class="sig-prename descclassname">nets.nn.modules.conv.</code><code class="sig-name descname">Conv2d</code><span class="sig-paren">(</span><em class="sig-param">in_channels</em>, <em class="sig-param">out_channels</em>, <em class="sig-param">filter_size</em>, <em class="sig-param">stride=1</em>, <em class="sig-param">pad=0</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/nn/modules/conv.html#Conv2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.nn.modules.conv.Conv2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Convolutional Neural Networks (CNN) are a class of Neural Networks that use convolution filters.
Their particularity is their ability to synthesis information and learn spatial features.
They are mainly used in Image Analysis, but are also known as <em>sliding windows</em> in Natural Language Processing.</p>
<p><code class="docutils literal notranslate"><span class="pre">Conv2d</span></code> networks applies a 2-d convolution on a 4-d tensor.</p>
<dl class="method">
<dt id="nets.nn.modules.conv.Conv2d.backward">
<code class="sig-name descname">backward</code><span class="sig-paren">(</span><em class="sig-param">dout</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/nn/modules/conv.html#Conv2d.backward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.nn.modules.conv.Conv2d.backward" title="Permalink to this definition">¶</a></dt>
<dd><p>One backward step.</p>
</dd></dl>

<dl class="method">
<dt id="nets.nn.modules.conv.Conv2d.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">inputs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/nn/modules/conv.html#Conv2d.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.nn.modules.conv.Conv2d.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>One forward step. Gradients and outputs should be saved in the <code class="docutils literal notranslate"><span class="pre">_cache</span></code> when training, to be able to
perform the backward pass.</p>
</dd></dl>

<dl class="method">
<dt id="nets.nn.modules.conv.Conv2d.inner_repr">
<code class="sig-name descname">inner_repr</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/nn/modules/conv.html#Conv2d.inner_repr"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.nn.modules.conv.Conv2d.inner_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>Display the inner parameter of a CNN</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-nets.nn.modules.dnn">
<span id="nets-nn-dnn"></span><h2>nets.nn.dnn<a class="headerlink" href="#module-nets.nn.modules.dnn" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="nets.nn.modules.dnn.DNN">
<em class="property">class </em><code class="sig-prename descclassname">nets.nn.modules.dnn.</code><code class="sig-name descname">DNN</code><span class="sig-paren">(</span><em class="sig-param">layer_dimensions</em>, <em class="sig-param">activation_hidden=ReLU(    (parameters):  )</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/nn/modules/dnn.html#DNN"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.nn.modules.dnn.DNN" title="Permalink to this definition">¶</a></dt>
<dd><p>Dense Neural Network.</p>
<dl class="attribute">
<dt id="nets.nn.modules.dnn.DNN.layer_dimensions">
<code class="sig-name descname">layer_dimensions</code><a class="headerlink" href="#nets.nn.modules.dnn.DNN.layer_dimensions" title="Permalink to this definition">¶</a></dt>
<dd><p><code class="docutils literal notranslate"><span class="pre">list</span></code> of <code class="docutils literal notranslate"><span class="pre">int</span></code> containing all layer dimensions. The dimension at index <code class="docutils literal notranslate"><span class="pre">i</span></code>
is the value of <code class="docutils literal notranslate"><span class="pre">layer_dimensions</span></code> at this index.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list(int)</p>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="nets.nn.modules.dnn.DNN.hidden_dimensions">
<code class="sig-name descname">hidden_dimensions</code><a class="headerlink" href="#nets.nn.modules.dnn.DNN.hidden_dimensions" title="Permalink to this definition">¶</a></dt>
<dd><p><code class="docutils literal notranslate"><span class="pre">list</span></code> of <code class="docutils literal notranslate"><span class="pre">int</span></code> containing only hidden dimensions.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list(int)</p>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="nets.nn.modules.dnn.DNN.training">
<code class="sig-name descname">training</code><a class="headerlink" href="#nets.nn.modules.dnn.DNN.training" title="Permalink to this definition">¶</a></dt>
<dd><p>Boolean to indicate if we are training or not. This function can namely be
used for inference only, in which case we do not need to store the features
values.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>bool</p>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="nets.nn.modules.dnn.DNN.activation_hidden">
<code class="sig-name descname">activation_hidden</code><a class="headerlink" href="#nets.nn.modules.dnn.DNN.activation_hidden" title="Permalink to this definition">¶</a></dt>
<dd><p>activation function used in hidden layers.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><a class="reference internal" href="#nets.nn.activation.Activation" title="nets.nn.activation.Activation">Activation</a></p>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="nets.nn.modules.dnn.DNN.activation_output">
<code class="sig-name descname">activation_output</code><a class="headerlink" href="#nets.nn.modules.dnn.DNN.activation_output" title="Permalink to this definition">¶</a></dt>
<dd><p>activation function used in the last layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><a class="reference internal" href="#nets.nn.activation.Activation" title="nets.nn.activation.Activation">Activation</a></p>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="nets.nn.modules.dnn.DNN._parameters">
<code class="sig-name descname">_parameters</code><a class="headerlink" href="#nets.nn.modules.dnn.DNN._parameters" title="Permalink to this definition">¶</a></dt>
<dd><p><code class="docutils literal notranslate"><span class="pre">dict</span></code> containing values of weights and biases for each layers.
- <code class="docutils literal notranslate"><span class="pre">weight_{i}</span></code>: contains the weight matrix of layer <code class="docutils literal notranslate"><span class="pre">i</span></code>
- <code class="docutils literal notranslate"><span class="pre">bias_{i}</span></code>: contains the bias array of layer <code class="docutils literal notranslate"><span class="pre">i</span></code></p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>dict</p>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="nets.nn.modules.dnn.DNN._cache">
<code class="sig-name descname">_cache</code><a class="headerlink" href="#nets.nn.modules.dnn.DNN._cache" title="Permalink to this definition">¶</a></dt>
<dd><p><code class="docutils literal notranslate"><span class="pre">dict</span></code> with
- the linear combinations Z^[l] = W^[l]a^[l-1] + b^[l] for l in [1, L].
- the activations A^[l] = activation(Z^[l]) for l in [1, L].
We cache them in order to use them when computing gradients in the back propagation.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>dict</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="nets.nn.modules.dnn.DNN.backward">
<code class="sig-name descname">backward</code><span class="sig-paren">(</span><em class="sig-param">outputs</em>, <em class="sig-param">labels</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/nn/modules/dnn.html#DNN.backward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.nn.modules.dnn.DNN.backward" title="Permalink to this definition">¶</a></dt>
<dd><p>Update parameters using backpropagation algorithm.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>outputs</strong> (<em>numpy.array</em>) – matrix of floats with shape (num_classes, batch_size).</p></li>
<li><p><strong>labels</strong> (<em>numpy.array</em>) – numpy array of integers with shape (num_classes, batch_size).
Collection of one-hot encoded true input labels.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><dl class="simple">
<dt>Dictionary with matrices that is to be used in the parameter update. Contains</dt><dd><ul class="simple">
<li><p>the gradient of the weights, grad_W^[l] for l in [1, L].</p></li>
<li><p>the gradient of the biases grad_b^[l] for l in [1, L].</p></li>
</ul>
</dd>
</dl>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>grad_params</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="nets.nn.modules.dnn.DNN.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">inputs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/nn/modules/dnn.html#DNN.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.nn.modules.dnn.DNN.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>One forward step.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>inputs</strong> (<em>numpy.array</em>) – float numpy array with shape (n^[0], batch_size). Input image batch.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><dl class="simple">
<dt>float numpy array with shape (n^[L], batch_size). The output predictions of the</dt><dd><p>network, where n^[L] is the number of prediction classes. For each input i in the batch,
output[c, i] gives the probability that input <code class="docutils literal notranslate"><span class="pre">i</span></code> belongs to class <code class="docutils literal notranslate"><span class="pre">c</span></code>.</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>outputs (numpy.array)</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="nets.nn.modules.dnn.DNN.init_parameters">
<code class="sig-name descname">init_parameters</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/nn/modules/dnn.html#DNN.init_parameters"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.nn.modules.dnn.DNN.init_parameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialize the parameters dictionary.
For Dense Neural Network, the parameters are either weights or biases. They are saved in the dictionary
with the following keys:</p>
<blockquote>
<div><ul class="simple">
<li><p>w_{i}: weight matrix at layer i,</p></li>
<li><p>b_{i}: bias vector at layer i.</p></li>
</ul>
</div></blockquote>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-nets.nn.modules.dropout">
<span id="nets-nn-dropout"></span><h2>nets.nn.dropout<a class="headerlink" href="#module-nets.nn.modules.dropout" title="Permalink to this headline">¶</a></h2>
<p>Define a <code class="docutils literal notranslate"><span class="pre">Dropout</span></code> layer.</p>
<dl class="class">
<dt id="nets.nn.modules.dropout.Dropout">
<em class="property">class </em><code class="sig-prename descclassname">nets.nn.modules.dropout.</code><code class="sig-name descname">Dropout</code><span class="sig-paren">(</span><em class="sig-param">prob=0.5</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/nn/modules/dropout.html#Dropout"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.nn.modules.dropout.Dropout" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a <code class="docutils literal notranslate"><span class="pre">dropout</span></code> to an input <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> with probability <code class="docutils literal notranslate"><span class="pre">prob</span></code>.
The effect of this layer is to zeros elements of the incoming tensors,
and cancel some neighbours effect / interactions from one layer to another.</p>
<dl class="method">
<dt id="nets.nn.modules.dropout.Dropout.backward">
<code class="sig-name descname">backward</code><span class="sig-paren">(</span><em class="sig-param">outputs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/nn/modules/dropout.html#Dropout.backward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.nn.modules.dropout.Dropout.backward" title="Permalink to this definition">¶</a></dt>
<dd><p>One backward step.</p>
</dd></dl>

<dl class="method">
<dt id="nets.nn.modules.dropout.Dropout.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">inputs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/nn/modules/dropout.html#Dropout.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.nn.modules.dropout.Dropout.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>One forward step. Gradients and outputs should be saved in the <code class="docutils literal notranslate"><span class="pre">_cache</span></code> when training, to be able to
perform the backward pass.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-nets.nn.modules.linear">
<span id="nets-nn-linear"></span><h2>nets.nn.linear<a class="headerlink" href="#module-nets.nn.modules.linear" title="Permalink to this headline">¶</a></h2>
<p>Linear layers are widely used either for Dense Neural Module or Convolutional Neural Module, to cite the most
popular. The architecture is made of two sets of <code class="docutils literal notranslate"><span class="pre">neurons</span></code> aka <code class="docutils literal notranslate"><span class="pre">perceptrons</span></code>, all connected with weights and biases.</p>
<dl class="class">
<dt id="nets.nn.modules.linear.Linear">
<em class="property">class </em><code class="sig-prename descclassname">nets.nn.modules.linear.</code><code class="sig-name descname">Linear</code><span class="sig-paren">(</span><em class="sig-param">input_dim</em>, <em class="sig-param">output_dim</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/nn/modules/linear.html#Linear"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.nn.modules.linear.Linear" title="Permalink to this definition">¶</a></dt>
<dd><p>A linear layer is made of a weight matrix <span class="math notranslate nohighlight">\(W\)</span> of shape <span class="math notranslate nohighlight">\((\text{input_dim}, \text{output_dim})\)</span>
and a bias vector <span class="math notranslate nohighlight">\(b\)</span> of shape <span class="math notranslate nohighlight">\((\text{output_dim})\)</span>. The linear transformation for an incoming vector
<span class="math notranslate nohighlight">\(x\)</span> of shape <span class="math notranslate nohighlight">\((N, \text{input_dim})\)</span> results in a vector <span class="math notranslate nohighlight">\(y\)</span> of shape <span class="math notranslate nohighlight">\((N, \text{output_dim})\)</span>:</p>
<div class="math notranslate nohighlight">
\[y = x W + b\]</div>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">batch_input</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span>
<span class="gp">... </span>                        <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="o">-</span><span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="o">-</span><span class="mi">3</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">batch_pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">batch_input</span><span class="p">)</span>
</pre></div>
</div>
<dl class="method">
<dt id="nets.nn.modules.linear.Linear.backward">
<code class="sig-name descname">backward</code><span class="sig-paren">(</span><em class="sig-param">dout</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/nn/modules/linear.html#Linear.backward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.nn.modules.linear.Linear.backward" title="Permalink to this definition">¶</a></dt>
<dd><p>Backward pass for a single Linear layer.</p>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>input (Tensor): upstream gradient.</p></li>
<li><p>output (Tensor): downstream gradient after a linear transformation.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="nets.nn.modules.linear.Linear.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">x</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/nn/modules/linear.html#Linear.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.nn.modules.linear.Linear.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward pass. Compute the linear transformation and save the results in the <cite>_cache</cite>, which will be
used during the backward pass.</p>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>input (numpy.array): batch inputs of shape.</p></li>
<li><dl class="simple">
<dt>output (numpy.array): results of the linear transformation,</dt><dd><p>of shape <span class="math notranslate nohighlight">\((N, \text{input_dim})\)</span>, with <span class="math notranslate nohighlight">\(N = \text{batch_size}\)</span>.</p>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="nets.nn.modules.linear.Linear.init_params">
<code class="sig-name descname">init_params</code><span class="sig-paren">(</span><em class="sig-param">mode=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/nn/modules/linear.html#Linear.init_params"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.nn.modules.linear.Linear.init_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialize the parameters dictionary.
For Dense Neural Module, the parameters are either weights or biases. They are saved in the dictionary
<cite>_params</cite> with the following keys: weight matrix <code class="docutils literal notranslate"><span class="pre">w</span></code>, bias vector <code class="docutils literal notranslate"><span class="pre">b</span></code>.
The initialization can be changed with <code class="docutils literal notranslate"><span class="pre">mode</span></code> parameter, between the default uniform <span class="math notranslate nohighlight">\(\mathcal{U}(0, 1)\)</span>
initialization or use <span class="math notranslate nohighlight">\(\text{He et al.}  \quad \mathcal{N} (0, \frac{1}{input_dim})\)</span>.</p>
</dd></dl>

<dl class="method">
<dt id="nets.nn.modules.linear.Linear.inner_repr">
<code class="sig-name descname">inner_repr</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/nn/modules/linear.html#Linear.inner_repr"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.nn.modules.linear.Linear.inner_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>Display the inner parameter of a Linear layer</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-nets.nn.modules.module">
<span id="nets-nn-module"></span><h2>nets.nn.module<a class="headerlink" href="#module-nets.nn.modules.module" title="Permalink to this headline">¶</a></h2>
<p>Modules are the main architecture for all transformations from one tensor to another. In other words,
a neural network is a succession of modules (layer, convolution, activation…).
When building a custom neural network, your model must inherits from <code class="docutils literal notranslate"><span class="pre">Module</span></code> abstract class and override the
<code class="docutils literal notranslate"><span class="pre">forward</span></code> method. Moreover, you can specify the back-propagation rule in <code class="docutils literal notranslate"><span class="pre">backward</span></code> method. Usually, the
<code class="docutils literal notranslate"><span class="pre">backward</span></code> method computes the naive back-propagation using only local gradients saved in the modules’s <code class="docutils literal notranslate"><span class="pre">_cache</span></code>.
If you don’t specify it, <strong>NETS</strong> will uses <code class="docutils literal notranslate"><span class="pre">autograd</span></code> functionality to compute all gradients.</p>
<dl class="class">
<dt id="nets.nn.modules.module.Module">
<em class="property">class </em><code class="sig-prename descclassname">nets.nn.modules.module.</code><code class="sig-name descname">Module</code><a class="reference internal" href="../_modules/nets/nn/modules/module.html#Module"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.nn.modules.module.Module" title="Permalink to this definition">¶</a></dt>
<dd><p>Abstract Module architecture. All models used to transform tensors should extends from this class to benefits
<code class="docutils literal notranslate"><span class="pre">forward</span></code> and <code class="docutils literal notranslate"><span class="pre">backward</span></code> propagation rules.</p>
<dl class="method">
<dt id="nets.nn.modules.module.Module.add">
<code class="sig-name descname">add</code><span class="sig-paren">(</span><em class="sig-param">*modules</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/nn/modules/module.html#Module.add"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.nn.modules.module.Module.add" title="Permalink to this definition">¶</a></dt>
<dd><p>Add modules to the current one.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>modules</strong> (<a class="reference internal" href="#nets.nn.modules.module.Module" title="nets.nn.modules.module.Module"><em>Module</em></a>) – modules to add</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="nets.nn.modules.module.Module.backward">
<code class="sig-name descname">backward</code><span class="sig-paren">(</span><em class="sig-param">*outputs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/nn/modules/module.html#Module.backward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.nn.modules.module.Module.backward" title="Permalink to this definition">¶</a></dt>
<dd><p>One backward step.</p>
</dd></dl>

<dl class="method">
<dt id="nets.nn.modules.module.Module.cache">
<code class="sig-name descname">cache</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/nn/modules/module.html#Module.cache"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.nn.modules.module.Module.cache" title="Permalink to this definition">¶</a></dt>
<dd><p>Iterator through all cache dict</p>
</dd></dl>

<dl class="method">
<dt id="nets.nn.modules.module.Module.eval">
<code class="sig-name descname">eval</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/nn/modules/module.html#Module.eval"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.nn.modules.module.Module.eval" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the <code class="docutils literal notranslate"><span class="pre">training</span></code> attribute to evaluation mode.</p>
</dd></dl>

<dl class="method">
<dt id="nets.nn.modules.module.Module.forward">
<em class="property">abstract </em><code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">*inputs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/nn/modules/module.html#Module.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.nn.modules.module.Module.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>One forward step. Gradients and outputs should be saved in the <code class="docutils literal notranslate"><span class="pre">_cache</span></code> when training, to be able to
perform the backward pass.</p>
</dd></dl>

<dl class="method">
<dt id="nets.nn.modules.module.Module.get_name">
<code class="sig-name descname">get_name</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/nn/modules/module.html#Module.get_name"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.nn.modules.module.Module.get_name" title="Permalink to this definition">¶</a></dt>
<dd><p>Quick access to get the name of a modules.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>modules’s name</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>string</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="nets.nn.modules.module.Module.gradients">
<code class="sig-name descname">gradients</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/nn/modules/module.html#Module.gradients"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.nn.modules.module.Module.gradients" title="Permalink to this definition">¶</a></dt>
<dd><p>Iterator through all gradients</p>
</dd></dl>

<dl class="method">
<dt id="nets.nn.modules.module.Module.inner_repr">
<code class="sig-name descname">inner_repr</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/nn/modules/module.html#Module.inner_repr"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.nn.modules.module.Module.inner_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the representation of a single modules.
This method should be unique for each modules.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>the representation of one modules.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>string</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="nets.nn.modules.module.Module.load_state">
<code class="sig-name descname">load_state</code><span class="sig-paren">(</span><em class="sig-param">state_dict</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/nn/modules/module.html#Module.load_state"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.nn.modules.module.Module.load_state" title="Permalink to this definition">¶</a></dt>
<dd><p>Load parameters from a <code class="docutils literal notranslate"><span class="pre">state_dict</span></code> dictionary.</p>
</dd></dl>

<dl class="method">
<dt id="nets.nn.modules.module.Module.modules">
<code class="sig-name descname">modules</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/nn/modules/module.html#Module.modules"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.nn.modules.module.Module.modules" title="Permalink to this definition">¶</a></dt>
<dd><p>Iterator through all gradients</p>
</dd></dl>

<dl class="method">
<dt id="nets.nn.modules.module.Module.parameters">
<code class="sig-name descname">parameters</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/nn/modules/module.html#Module.parameters"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.nn.modules.module.Module.parameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Iterator through all parameters</p>
</dd></dl>

<dl class="method">
<dt id="nets.nn.modules.module.Module.save">
<code class="sig-name descname">save</code><span class="sig-paren">(</span><em class="sig-param">filename='model.pickle'</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/nn/modules/module.html#Module.save"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.nn.modules.module.Module.save" title="Permalink to this definition">¶</a></dt>
<dd><p>Save a model as a PICKLE file.</p>
</dd></dl>

<dl class="method">
<dt id="nets.nn.modules.module.Module.save_dict">
<code class="sig-name descname">save_dict</code><span class="sig-paren">(</span><em class="sig-param">filename='state_dict.json'</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/nn/modules/module.html#Module.save_dict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.nn.modules.module.Module.save_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>Save the state as a JSON file.</p>
</dd></dl>

<dl class="method">
<dt id="nets.nn.modules.module.Module.state_dict">
<code class="sig-name descname">state_dict</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/nn/modules/module.html#Module.state_dict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.nn.modules.module.Module.state_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>Save all parameters in a dictionary.</p>
</dd></dl>

<dl class="method">
<dt id="nets.nn.modules.module.Module.train">
<code class="sig-name descname">train</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/nn/modules/module.html#Module.train"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.nn.modules.module.Module.train" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the <code class="docutils literal notranslate"><span class="pre">training</span></code> attribute to training mode.</p>
</dd></dl>

<dl class="method">
<dt id="nets.nn.modules.module.Module.zero_grad">
<code class="sig-name descname">zero_grad</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/nn/modules/module.html#Module.zero_grad"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.nn.modules.module.Module.zero_grad" title="Permalink to this definition">¶</a></dt>
<dd><p>Zero grad all parameters within a modules</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-nets.nn.modules.pool">
<span id="nets-nn-pool"></span><h2>nets.nn.pool<a class="headerlink" href="#module-nets.nn.modules.pool" title="Permalink to this headline">¶</a></h2>
<p>This modules defines a <code class="docutils literal notranslate"><span class="pre">Pooling</span></code> layer. Usually such layer is used after a convolutional layer.</p>
<dl class="class">
<dt id="nets.nn.modules.pool.MaxPool2d">
<em class="property">class </em><code class="sig-prename descclassname">nets.nn.modules.pool.</code><code class="sig-name descname">MaxPool2d</code><span class="sig-paren">(</span><em class="sig-param">pool_size</em>, <em class="sig-param">stride=1</em>, <em class="sig-param">pad=0</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/nn/modules/pool.html#MaxPool2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.nn.modules.pool.MaxPool2d" title="Permalink to this definition">¶</a></dt>
<dd><p>A <code class="docutils literal notranslate"><span class="pre">Pooling</span></code> layer extract features from a multi dimensional <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> and map them into another one.
This extraction is used to decrease the dimension of the input, and often used after a convolutional layer.</p>
<dl class="method">
<dt id="nets.nn.modules.pool.MaxPool2d.backward">
<code class="sig-name descname">backward</code><span class="sig-paren">(</span><em class="sig-param">dout</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/nn/modules/pool.html#MaxPool2d.backward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.nn.modules.pool.MaxPool2d.backward" title="Permalink to this definition">¶</a></dt>
<dd><p>Manual backward pass for a MaxPool2d layer.</p>
</dd></dl>

<dl class="method">
<dt id="nets.nn.modules.pool.MaxPool2d.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">x</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/nn/modules/pool.html#MaxPool2d.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.nn.modules.pool.MaxPool2d.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward pass.</p>
</dd></dl>

<dl class="method">
<dt id="nets.nn.modules.pool.MaxPool2d.inner_repr">
<code class="sig-name descname">inner_repr</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/nn/modules/pool.html#MaxPool2d.inner_repr"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.nn.modules.pool.MaxPool2d.inner_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>Display the inner parameter of a CNN</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-nets.nn.modules.rnn">
<span id="nets-nn-rnn"></span><h2>nets.nn.rnn<a class="headerlink" href="#module-nets.nn.modules.rnn" title="Permalink to this headline">¶</a></h2>
<p>Defines a basic Recurrent Neural Network.</p>
<dl class="class">
<dt id="nets.nn.modules.rnn.RNN">
<em class="property">class </em><code class="sig-prename descclassname">nets.nn.modules.rnn.</code><code class="sig-name descname">RNN</code><span class="sig-paren">(</span><em class="sig-param">input_dim</em>, <em class="sig-param">hidden_dim</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/nn/modules/rnn.html#RNN"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.nn.modules.rnn.RNN" title="Permalink to this definition">¶</a></dt>
<dd><p>Recurrent neural network (RNN) is a type of neural network that has been successful in modelling sequential data,
e.g. language, speech, protein sequences, etc.</p>
<p>A RNN performs its computations in a cyclic manner, where the same computation is applied to every sample
of a given sequence. The idea is that the network should be able to use the previous computations as some form
of memory and apply this to future computations.
From the [exercise 02456 from DTU course](<a class="reference external" href="https://github.com/DeepLearningDTU/02456-deep-learning-with-PyTorch">https://github.com/DeepLearningDTU/02456-deep-learning-with-PyTorch</a>).</p>
<dl class="method">
<dt id="nets.nn.modules.rnn.RNN.backward">
<code class="sig-name descname">backward</code><span class="sig-paren">(</span><em class="sig-param">dout</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/nn/modules/rnn.html#RNN.backward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.nn.modules.rnn.RNN.backward" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the backward pass of a vanilla RNN.
Save gradients parameters in the <code class="docutils literal notranslate"><span class="pre">_grads</span></code> parameter.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>dout</strong> (<a class="reference internal" href="#nets.tensor.Tensor" title="nets.tensor.Tensor"><em>Tensor</em></a>) – upstream gradient.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>downstream gradient</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="#nets.tensor.Tensor" title="nets.tensor.Tensor">Tensor</a></p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="nets.nn.modules.rnn.RNN.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">inputs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/nn/modules/rnn.html#RNN.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.nn.modules.rnn.RNN.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the forward pass of a vanilla RNN.</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}h_{0} = 0\\h_t =       ext{tanh}(x W_{ih} + h_{t-1} W_{hh} + b_{h})\\y = h_t W_{ho} + b_{o}\end{aligned}\end{align} \]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>inputs</strong> – sequence of inputs to be processed</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="nets.nn.modules.rnn.RNN.set_hidden_0">
<code class="sig-name descname">set_hidden_0</code><span class="sig-paren">(</span><em class="sig-param">hidden_cell</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/nn/modules/rnn.html#RNN.set_hidden_0"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.nn.modules.rnn.RNN.set_hidden_0" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the first hidden cell.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-nets.nn.modules.rnnbase">
<span id="nets-nn-rnnbase"></span><h2>nets.nn.rnnbase<a class="headerlink" href="#module-nets.nn.modules.rnnbase" title="Permalink to this headline">¶</a></h2>
<p>Defines the general structure of a Recurrent Network, which can takes the form of a:</p>
<ul class="simple">
<li><p>Recurrent Neural Network,</p></li>
<li><p>Long Short Term Memory,</p></li>
<li><p>Gated Recurrent Unit.</p></li>
</ul>
<dl class="class">
<dt id="nets.nn.modules.rnnbase.RNNBase">
<em class="property">class </em><code class="sig-prename descclassname">nets.nn.modules.rnnbase.</code><code class="sig-name descname">RNNBase</code><span class="sig-paren">(</span><em class="sig-param">*args</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/nn/modules/rnnbase.html#RNNBase"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.nn.modules.rnnbase.RNNBase" title="Permalink to this definition">¶</a></dt>
<dd><p>Base architecture for Recurrent Networks.
Every general recurrent layers should extends this class to inherits private attributes and
helper methods.</p>
</dd></dl>

</div>
<div class="section" id="module-nets.nn.modules.sequential">
<span id="nets-nn-sequential"></span><h2>nets.nn.sequential<a class="headerlink" href="#module-nets.nn.modules.sequential" title="Permalink to this headline">¶</a></h2>
<p><code class="docutils literal notranslate"><span class="pre">Sequential</span></code> models are an ordered succession of modules.</p>
<dl class="class">
<dt id="nets.nn.modules.sequential.Sequential">
<em class="property">class </em><code class="sig-prename descclassname">nets.nn.modules.sequential.</code><code class="sig-name descname">Sequential</code><span class="sig-paren">(</span><em class="sig-param">*modules</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/nn/modules/sequential.html#Sequential"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.nn.modules.sequential.Sequential" title="Permalink to this definition">¶</a></dt>
<dd><p>A <code class="docutils literal notranslate"><span class="pre">Sequential</span></code> model is build from a succession of <code class="docutils literal notranslate"><span class="pre">Modules</span></code>. All of them <strong>must</strong> have a
<code class="docutils literal notranslate"><span class="pre">forward</span></code> method. In addition, a <code class="docutils literal notranslate"><span class="pre">backward</span></code> pass is created by default, running the back-propagation in all
modules previously added. If for one of the modules the <code class="docutils literal notranslate"><span class="pre">backward</span></code> pass is not implemented, it will raise an error.</p>
<dl class="method">
<dt id="nets.nn.modules.sequential.Sequential.backward">
<code class="sig-name descname">backward</code><span class="sig-paren">(</span><em class="sig-param">grad</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/nn/modules/sequential.html#Sequential.backward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.nn.modules.sequential.Sequential.backward" title="Permalink to this definition">¶</a></dt>
<dd><p>Vanilla backward pass. This pass computes local gradients from <code class="docutils literal notranslate"><span class="pre">parameters</span></code> saved in its <code class="docutils literal notranslate"><span class="pre">_cache</span></code>.</p>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>inputs (Tensor): upstream gradient. The first downstream gradient is usually the <code class="docutils literal notranslate"><span class="pre">loss</span></code>.</p></li>
<li><p>outputs (Tensor): last downstream gradient.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="nets.nn.modules.sequential.Sequential.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">inputs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/nn/modules/sequential.html#Sequential.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.nn.modules.sequential.Sequential.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the forward pass for all modules within the sequential modules.</p>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>inputs (Tensor): incoming data.</p></li>
<li><p>outputs (Tensor): result of all forward pass.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
</div>
<div class="section" id="nets-optim">
<h1>nets.optim<a class="headerlink" href="#nets-optim" title="Permalink to this headline">¶</a></h1>
<div class="section" id="module-nets.optim.optimizer">
<span id="nets-optim-optimizer"></span><h2>nets.optim.optimizer<a class="headerlink" href="#module-nets.optim.optimizer" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="nets.optim.optimizer.Optimizer">
<em class="property">class </em><code class="sig-prename descclassname">nets.optim.optimizer.</code><code class="sig-name descname">Optimizer</code><span class="sig-paren">(</span><em class="sig-param">parameters</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/optim/optimizer.html#Optimizer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.optim.optimizer.Optimizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Optimizer. Modify a model’s parameters, and update its weights / biases.</p>
<dl class="method">
<dt id="nets.optim.optimizer.Optimizer.backward">
<code class="sig-name descname">backward</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/optim/optimizer.html#Optimizer.backward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.optim.optimizer.Optimizer.backward" title="Permalink to this definition">¶</a></dt>
<dd><p>Update rules without <code class="docutils literal notranslate"><span class="pre">autograd</span></code></p>
</dd></dl>

<dl class="method">
<dt id="nets.optim.optimizer.Optimizer.step">
<em class="property">abstract </em><code class="sig-name descname">step</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/optim/optimizer.html#Optimizer.step"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.optim.optimizer.Optimizer.step" title="Permalink to this definition">¶</a></dt>
<dd><p>Update the parameters. Should be used only with <code class="docutils literal notranslate"><span class="pre">autograd</span></code> system</p>
</dd></dl>

<dl class="method">
<dt id="nets.optim.optimizer.Optimizer.zero_grad">
<code class="sig-name descname">zero_grad</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/optim/optimizer.html#Optimizer.zero_grad"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.optim.optimizer.Optimizer.zero_grad" title="Permalink to this definition">¶</a></dt>
<dd><p>Zero grad all parameters contained in <code class="docutils literal notranslate"><span class="pre">parameters</span></code> attribute.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>None</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-nets.optim.adagrad">
<span id="nets-optim-adagrad"></span><h2>nets.optim.adagrad<a class="headerlink" href="#module-nets.optim.adagrad" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="nets.optim.adagrad.Adagrad">
<em class="property">class </em><code class="sig-prename descclassname">nets.optim.adagrad.</code><code class="sig-name descname">Adagrad</code><span class="sig-paren">(</span><em class="sig-param">parameters</em>, <em class="sig-param">lr=0.01</em>, <em class="sig-param">epsilon=1e-08</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/optim/adagrad.html#Adagrad"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.optim.adagrad.Adagrad" title="Permalink to this definition">¶</a></dt>
<dd><p>Adagrad optimizer. More sophisticated version of <code class="docutils literal notranslate"><span class="pre">SGD</span></code>, with learning rate decay.</p>
<dl class="method">
<dt id="nets.optim.adagrad.Adagrad.step">
<code class="sig-name descname">step</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/optim/adagrad.html#Adagrad.step"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.optim.adagrad.Adagrad.step" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs Adagrad update rules.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-nets.optim.adam">
<span id="nets-optim-adam"></span><h2>nets.optim.adam<a class="headerlink" href="#module-nets.optim.adam" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="nets.optim.adam.Adam">
<em class="property">class </em><code class="sig-prename descclassname">nets.optim.adam.</code><code class="sig-name descname">Adam</code><span class="sig-paren">(</span><em class="sig-param">parameters</em>, <em class="sig-param">lr=0.01</em>, <em class="sig-param">beta1=0.9</em>, <em class="sig-param">beta2=0.999</em>, <em class="sig-param">epsilon=1e-08</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/optim/adam.html#Adam"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.optim.adam.Adam" title="Permalink to this definition">¶</a></dt>
<dd><p>Uses the Adam update rule, which incorporates moving averages of both the
gradient and its square and a bias correction term.</p>
<dl class="method">
<dt id="nets.optim.adam.Adam.step">
<code class="sig-name descname">step</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/optim/adam.html#Adam.step"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.optim.adam.Adam.step" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs Adam update rules.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-nets.optim.rmsprop">
<span id="nets-optim-rmsprop"></span><h2>nets.optim.rmsprop<a class="headerlink" href="#module-nets.optim.rmsprop" title="Permalink to this headline">¶</a></h2>
<p>Uses the RMSProp update rule, which uses a moving average of squared gradient values to set adaptive per-parameter
learning rates.</p>
<dl class="class">
<dt id="nets.optim.rmsprop.RMSprop">
<em class="property">class </em><code class="sig-prename descclassname">nets.optim.rmsprop.</code><code class="sig-name descname">RMSprop</code><span class="sig-paren">(</span><em class="sig-param">parameters</em>, <em class="sig-param">lr=0.01</em>, <em class="sig-param">decay=0.99</em>, <em class="sig-param">epsilon=1e-08</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/optim/rmsprop.html#RMSprop"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.optim.rmsprop.RMSprop" title="Permalink to this definition">¶</a></dt>
<dd><p>Uses the RMSProp update rule, which uses a moving average of squared gradient values to set adaptive per-parameter
learning rates.</p>
<dl class="method">
<dt id="nets.optim.rmsprop.RMSprop.step">
<code class="sig-name descname">step</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/optim/rmsprop.html#RMSprop.step"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.optim.rmsprop.RMSprop.step" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs RMSProp update.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-nets.optim.sgd">
<span id="nets-optim-sgd"></span><h2>nets.optim.sgd<a class="headerlink" href="#module-nets.optim.sgd" title="Permalink to this headline">¶</a></h2>
<p>Stochastic Gradient Descent is a popular optimizer for machine learning purposes.</p>
<dl class="class">
<dt id="nets.optim.sgd.SGD">
<em class="property">class </em><code class="sig-prename descclassname">nets.optim.sgd.</code><code class="sig-name descname">SGD</code><span class="sig-paren">(</span><em class="sig-param">parameters</em>, <em class="sig-param">lr=0.01</em>, <em class="sig-param">momentum=0</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/optim/sgd.html#SGD"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.optim.sgd.SGD" title="Permalink to this definition">¶</a></dt>
<dd><p>Stochastic Gradient Descent optimizer follows the parameters optimization:</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>A momentum can be added to this process.</p>
</div>
<dl class="method">
<dt id="nets.optim.sgd.SGD.step">
<code class="sig-name descname">step</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/optim/sgd.html#SGD.step"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.optim.sgd.SGD.step" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs stochastic gradient descent with momentum.</p>
</dd></dl>

</dd></dl>

</div>
</div>
<div class="section" id="nets-solver">
<h1>nets.solver<a class="headerlink" href="#nets-solver" title="Permalink to this headline">¶</a></h1>
<div class="section" id="module-nets.solver.solver">
<span id="nets-solver-solver"></span><h2>nets.solver.solver<a class="headerlink" href="#module-nets.solver.solver" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="nets.solver.solver.Solver">
<em class="property">class </em><code class="sig-prename descclassname">nets.solver.solver.</code><code class="sig-name descname">Solver</code><span class="sig-paren">(</span><em class="sig-param">model=None</em>, <em class="sig-param">criterion=None</em>, <em class="sig-param">optimizer=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/solver/solver.html#Solver"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.solver.solver.Solver" title="Permalink to this definition">¶</a></dt>
<dd><p>Train and evaluate models.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<a class="reference internal" href="#nets.nn.modules.module.Module" title="nets.nn.modules.module.Module"><em>Module</em></a>) – model to optimize or test.</p></li>
<li><p><strong>criterion</strong> (<a class="reference internal" href="#nets.nn.loss.Loss" title="nets.nn.loss.Loss"><em>Loss</em></a>) – loss function.</p></li>
<li><p><strong>optimizer</strong> (<a class="reference internal" href="#nets.optim.optimizer.Optimizer" title="nets.optim.optimizer.Optimizer"><em>Optimizer</em></a>) – optimizer for weights and biases.</p></li>
</ul>
</dd>
</dl>
<dl>
<dt>Attributes::</dt><dd><p>model (Module): model to optimize or test.
checkpoint (dict): checkpoint of the best model tested.
criterion (Loss): loss function.
optimizer (Optimizer): optimizer for weights and biases.
performance (dict):</p>
<blockquote>
<div><ul class="simple">
<li><dl class="simple">
<dt>train (dict):</dt><dd><ul>
<li><p>loss (list(float))</p></li>
<li><p>accuracy (list(float))</p></li>
<li><p>precision (list(float))</p></li>
<li><p>recall (list(float))</p></li>
<li><p>macro_f1 (list(float))</p></li>
<li><p>confusion_matrix (list(list(list(int))))</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>eval (dict):</dt><dd><ul>
<li><p>loss (list(float))</p></li>
<li><p>accuracy (list(float))</p></li>
<li><p>precision (list(float))</p></li>
<li><p>recall (list(float))</p></li>
<li><p>macro_f1 (list(float))</p></li>
<li><p>confusion_matrix (list(list(list(int))))</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</div></blockquote>
</dd>
</dl>
<dl class="method">
<dt id="nets.solver.solver.Solver.evaluate">
<em class="property">abstract </em><code class="sig-name descname">evaluate</code><span class="sig-paren">(</span><em class="sig-param">iterator</em>, <em class="sig-param">*args</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/solver/solver.html#Solver.evaluate"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.solver.solver.Solver.evaluate" title="Permalink to this definition">¶</a></dt>
<dd><p>Evaluate one time the model on iterator data.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>iterator</strong> (<a class="reference internal" href="#nets.data.iterator.Iterator" title="nets.data.iterator.Iterator"><em>Iterator</em></a>) – iterator containing batch samples of data.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the performance and metrics of the training session.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>dict</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="nets.solver.solver.Solver.get_accuracy">
<code class="sig-name descname">get_accuracy</code><span class="sig-paren">(</span><em class="sig-param">y_tilde</em>, <em class="sig-param">y</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/solver/solver.html#Solver.get_accuracy"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.solver.solver.Solver.get_accuracy" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute accuracy from predicted classes and gold labels.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_tilde</strong> (<a class="reference internal" href="#nets.tensor.Tensor" title="nets.tensor.Tensor"><em>Tensor</em></a>) – 1D tensor containing the predicted classes for each predictions
in the batch. This tensor should be computed through <cite>get_predicted_classes(y_hat)</cite> method.</p></li>
<li><p><strong>y</strong> (<a class="reference internal" href="#nets.tensor.Tensor" title="nets.tensor.Tensor"><em>Tensor</em></a>) – gold labels. Note that y_tilde an y must have the same shape.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the mean of correct answers.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="nets.solver.solver.Solver.reset">
<code class="sig-name descname">reset</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/solver/solver.html#Solver.reset"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.solver.solver.Solver.reset" title="Permalink to this definition">¶</a></dt>
<dd><p>Reset the performance dictionary.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>None</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="nets.solver.solver.Solver.run">
<code class="sig-name descname">run</code><span class="sig-paren">(</span><em class="sig-param">epochs</em>, <em class="sig-param">train_iterator</em>, <em class="sig-param">eval_iterator</em>, <em class="sig-param">*args</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/solver/solver.html#Solver.run"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.solver.solver.Solver.run" title="Permalink to this definition">¶</a></dt>
<dd><p>Train and evaluate a model X times. During the training, both training
and evaluation results are saved under the <cite>performance</cite> attribute.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>epochs</strong> (<em>int</em>) – number of times the model will be trained.</p></li>
<li><p><strong>train_iterator</strong> (<a class="reference internal" href="#nets.data.iterator.Iterator" title="nets.data.iterator.Iterator"><em>Iterator</em></a>) – iterator containing batch samples of data.</p></li>
<li><p><strong>eval_iterator</strong> (<a class="reference internal" href="#nets.data.iterator.Iterator" title="nets.data.iterator.Iterator"><em>Iterator</em></a>) – iterator containing batch samples of data.</p></li>
<li><p><strong>verbose</strong> (<em>bool</em><em>, </em><em>optional</em>) – if <cite>True</cite> display a progress bar and metrics at each epoch.
The default is True.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>None</p>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">performer</span> <span class="o">=</span> <span class="n">MySolver</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Train &amp; eval EPOCHS times</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">EPOCHS</span> <span class="o">=</span> <span class="mi">10</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">performer</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">EPOCHS</span><span class="p">,</span> <span class="n">train_iterator</span><span class="p">,</span> <span class="n">eval_iterator</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="go">    Epoch:        1/10</span>
<span class="go">    Training:     100% | [==================================================] | Time: 2m 26s</span>
<span class="go">    Validation:   100% | [==================================================] | Time: 0m 4s</span>
<span class="go">    Stats Training:    | Loss: 0.349 | Acc: 84.33% | Prec.: 84.26% | Rec.: 84.33% | F1: 84.26%</span>
<span class="go">    Stats Evaluation:  | Loss: 0.627 | Acc: 72.04% | Prec.: 72.22% | Rec.: 72.17% | F1: 72.22%</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># ...</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="nets.solver.solver.Solver.save">
<code class="sig-name descname">save</code><span class="sig-paren">(</span><em class="sig-param">filename=None</em>, <em class="sig-param">dirpath='.'</em>, <em class="sig-param">checkpoint=True</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/solver/solver.html#Solver.save"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.solver.solver.Solver.save" title="Permalink to this definition">¶</a></dt>
<dd><p>Save the best torch model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>filename</strong> (<em>str</em><em>, </em><em>optional</em>) – name of the model. The default is “model.pt”.</p></li>
<li><p><strong>dirpath</strong> (<em>str</em><em>, </em><em>optional</em>) – path to the desired foldre location. The default is “.”.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>None.</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="nets.solver.solver.Solver.train">
<em class="property">abstract </em><code class="sig-name descname">train</code><span class="sig-paren">(</span><em class="sig-param">iterator</em>, <em class="sig-param">*args</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/solver/solver.html#Solver.train"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.solver.solver.Solver.train" title="Permalink to this definition">¶</a></dt>
<dd><p>Train one time the model on iterator data.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>iterator</strong> (<a class="reference internal" href="#nets.data.iterator.Iterator" title="nets.data.iterator.Iterator"><em>Iterator</em></a>) – iterator containing batch samples of data.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the performance and metrics of the training session.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>dict</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="function">
<dt id="nets.solver.solver.get_metrics">
<code class="sig-prename descclassname">nets.solver.solver.</code><code class="sig-name descname">get_metrics</code><span class="sig-paren">(</span><em class="sig-param">gold_labels</em>, <em class="sig-param">predictions</em>, <em class="sig-param">labels=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/solver/solver.html#get_metrics"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.solver.solver.get_metrics" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute metrics for given predictions. This method returns basic
metrics as a dictionary.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>gold_labels</strong> (<a class="reference internal" href="#nets.tensor.Tensor" title="nets.tensor.Tensor"><em>Tensor</em></a>) – true labels.</p></li>
<li><p><strong>predictions</strong> (<a class="reference internal" href="#nets.tensor.Tensor" title="nets.tensor.Tensor"><em>Tensor</em></a>) – predicted labels.</p></li>
<li><p><strong>labels</strong> (<em>list</em><em>, </em><em>optional</em>) – list of classes indices.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>dict</p>
</dd>
</dl>
</dd></dl>

</div>
</div>
<div class="section" id="nets-data">
<h1>nets.data<a class="headerlink" href="#nets-data" title="Permalink to this headline">¶</a></h1>
<div class="section" id="module-nets.data.batch">
<span id="nets-data-batch"></span><h2>nets.data.batch<a class="headerlink" href="#module-nets.data.batch" title="Permalink to this headline">¶</a></h2>
<p>A <code class="docutils literal notranslate"><span class="pre">Batch</span></code> is a set of examples, usually normalized and scaled for faster predictions.</p>
<dl class="class">
<dt id="nets.data.batch.Batch">
<em class="property">class </em><code class="sig-prename descclassname">nets.data.batch.</code><code class="sig-name descname">Batch</code><span class="sig-paren">(</span><em class="sig-param">example</em>, <em class="sig-param">batch_size</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/data/batch.html#Batch"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.data.batch.Batch" title="Permalink to this definition">¶</a></dt>
<dd><p>A <code class="docutils literal notranslate"><span class="pre">Batch``depends</span> <span class="pre">on</span> <span class="pre">a</span> <span class="pre">``Dataset</span></code> and is made of <code class="docutils literal notranslate"><span class="pre">batch_size</span></code> <code class="docutils literal notranslate"><span class="pre">Example</span></code>.</p>
</dd></dl>

</div>
<div class="section" id="module-nets.data.dataset">
<span id="nets-data-dataset"></span><h2>nets.data.dataset<a class="headerlink" href="#module-nets.data.dataset" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="nets.data.dataset.Dataset">
<em class="property">class </em><code class="sig-prename descclassname">nets.data.dataset.</code><code class="sig-name descname">Dataset</code><span class="sig-paren">(</span><em class="sig-param">examples</em>, <em class="sig-param">fields</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/data/dataset.html#Dataset"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.data.dataset.Dataset" title="Permalink to this definition">¶</a></dt>
<dd><p>Abstract Dataset class. All dataset for machine learning purposes can inherits from this architecture,
for convenience.</p>
<dl class="method">
<dt id="nets.data.dataset.Dataset.download">
<em class="property">classmethod </em><code class="sig-name descname">download</code><span class="sig-paren">(</span><em class="sig-param">root</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/data/dataset.html#Dataset.download"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.data.dataset.Dataset.download" title="Permalink to this definition">¶</a></dt>
<dd><p>Download and unzip a web archive (.zip, .gz, or .tgz).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>root</strong> (<em>str</em>) – Folder to download data to.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Path to extracted dataset.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>string</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-nets.data.example">
<span id="nets-data-example"></span><h2>nets.data.example<a class="headerlink" href="#module-nets.data.example" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="nets.data.example.Example">
<em class="property">class </em><code class="sig-prename descclassname">nets.data.example.</code><code class="sig-name descname">Example</code><a class="reference internal" href="../_modules/nets/data/example.html#Example"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.data.example.Example" title="Permalink to this definition">¶</a></dt>
<dd><p>Store a single training / testing example, and store it as an attribute.
Highly inspired from PyTorch [example](<a class="reference external" href="https://github.com/pytorch/text/blob/master/torchtext/data/example.py">https://github.com/pytorch/text/blob/master/torchtext/data/example.py</a>)</p>
<dl class="method">
<dt id="nets.data.example.Example.fromlist">
<em class="property">classmethod </em><code class="sig-name descname">fromlist</code><span class="sig-paren">(</span><em class="sig-param">values</em>, <em class="sig-param">fields</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/data/example.html#Example.fromlist"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.data.example.Example.fromlist" title="Permalink to this definition">¶</a></dt>
<dd><p>Add an example from a list of data with respect to the fields.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>values</strong> – raw data</p></li>
<li><p><strong>fields</strong> (<em>tuple</em><em>(</em><em>string</em><em>, </em><a class="reference internal" href="#nets.data.example.Field" title="nets.data.example.Field"><em>Field</em></a><em>)</em>) – fields to preprocess the data on</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>None</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="nets.data.example.Field">
<em class="property">class </em><code class="sig-prename descclassname">nets.data.example.</code><code class="sig-name descname">Field</code><span class="sig-paren">(</span><em class="sig-param">transform=None</em>, <em class="sig-param">dtype=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/data/example.html#Field"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.data.example.Field" title="Permalink to this definition">¶</a></dt>
<dd><p>A <code class="docutils literal notranslate"><span class="pre">Field</span></code> defines the data to process from a raw dataset. It will convert the data into a tensor. The data can
be a string, integers, float etc. The data is meant to be preprocessed and the attribute <code class="docutils literal notranslate"><span class="pre">transform</span></code> handles
the way the user want to process the raw data.</p>
<dl class="method">
<dt id="nets.data.example.Field.process">
<code class="sig-name descname">process</code><span class="sig-paren">(</span><em class="sig-param">value</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/data/example.html#Field.process"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.data.example.Field.process" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the transformation and changes the data’s type if necessary.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>value</strong> (<a class="reference internal" href="#nets.tensor.Tensor" title="nets.tensor.Tensor"><em>Tensor</em></a>) – </p>
</dd>
</dl>
<p>Returns:</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-nets.data.iterator">
<span id="nets-data-iterator"></span><h2>nets.data.iterator<a class="headerlink" href="#module-nets.data.iterator" title="Permalink to this headline">¶</a></h2>
<p>This modules defines how the data should be called from a datasets.
This takes into account <code class="docutils literal notranslate"><span class="pre">shuffle</span></code> mode.</p>
<dl class="class">
<dt id="nets.data.iterator.Iterator">
<em class="property">class </em><code class="sig-prename descclassname">nets.data.iterator.</code><code class="sig-name descname">Iterator</code><span class="sig-paren">(</span><em class="sig-param">dataset</em>, <em class="sig-param">batch_size=32</em>, <em class="sig-param">shuffle=False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/data/iterator.html#Iterator"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.data.iterator.Iterator" title="Permalink to this definition">¶</a></dt>
<dd><p>An <code class="docutils literal notranslate"><span class="pre">Iterator</span></code> call the data in batches. These batches can be shuffled and normalized.
Usually, you want to feed a model with these batches.</p>
</dd></dl>

</div>
</div>
<div class="section" id="nets-datasets">
<h1>nets.datasets<a class="headerlink" href="#nets-datasets" title="Permalink to this headline">¶</a></h1>
<div class="section" id="module-nets.datasets.cifar">
<span id="nets-datasets-cifar"></span><h2>nets.datasets.cifar<a class="headerlink" href="#module-nets.datasets.cifar" title="Permalink to this headline">¶</a></h2>
<p>Load and preprocess the CIFAR-10 dataset.</p>
<dl class="class">
<dt id="nets.datasets.cifar.CIFAR10">
<em class="property">class </em><code class="sig-prename descclassname">nets.datasets.cifar.</code><code class="sig-name descname">CIFAR10</code><span class="sig-paren">(</span><em class="sig-param">filepath</em>, <em class="sig-param">transform=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/datasets/cifar.html#CIFAR10"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.datasets.cifar.CIFAR10" title="Permalink to this definition">¶</a></dt>
<dd><p>CIFAR-10 dataset, available at <a class="reference external" href="https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz">https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz</a></p>
<p>The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images
per class. There are 50000 training images and 10000 test images, with the following classes:</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 31%" />
<col style="width: 69%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Label</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>0</p></td>
<td><p>airplane</p></td>
</tr>
<tr class="row-odd"><td><p>1</p></td>
<td><p>automobile</p></td>
</tr>
<tr class="row-even"><td><p>2</p></td>
<td><p>bird</p></td>
</tr>
<tr class="row-odd"><td><p>3</p></td>
<td><p>cat</p></td>
</tr>
<tr class="row-even"><td><p>4</p></td>
<td><p>deer</p></td>
</tr>
<tr class="row-odd"><td><p>5</p></td>
<td><p>dog</p></td>
</tr>
<tr class="row-even"><td><p>6</p></td>
<td><p>frog</p></td>
</tr>
<tr class="row-odd"><td><p>7</p></td>
<td><p>horse</p></td>
</tr>
<tr class="row-even"><td><p>8</p></td>
<td><p>ship</p></td>
</tr>
<tr class="row-odd"><td><p>9</p></td>
<td><p>truck</p></td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The dataset is divided into five training batches and one test batch, each with 10000
images. The test batch contains exactly 1000 randomly-selected images from each class. The
training batches contain the remaining images in random order, but some training batches
may contain more images from one class than another. Between them, the training batches
contain exactly 5000 images from each class.</p>
</div>
<dl class="method">
<dt id="nets.datasets.cifar.CIFAR10.splits">
<em class="property">classmethod </em><code class="sig-name descname">splits</code><span class="sig-paren">(</span><em class="sig-param">root='.data'</em>, <em class="sig-param">train='data_batch_'</em>, <em class="sig-param">test='test_batch'</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/datasets/cifar.html#CIFAR10.splits"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.datasets.cifar.CIFAR10.splits" title="Permalink to this definition">¶</a></dt>
<dd><p>Loads training, validation, and test partitions of the cifar10 dataset
(<a class="reference external" href="https://www.cs.toronto.edu/~kriz/cifar.html">https://www.cs.toronto.edu/~kriz/cifar.html</a>). If the data is not already contained in
<code class="docutils literal notranslate"><span class="pre">root</span></code> folder, it will download it.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>root</strong> (<em>string</em>) – relative or absolute path of the dataset.</p></li>
<li><p><strong>train</strong> (<em>string</em>) – training data path</p></li>
<li><p><strong>test</strong> (<em>string</em>) – testing data path</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>training and testing datasets</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>tuple(<a class="reference internal" href="#nets.data.dataset.Dataset" title="nets.data.dataset.Dataset">Dataset</a>)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-nets.datasets.mnist">
<span id="nets-datasets-mnist"></span><h2>nets.datasets.mnist<a class="headerlink" href="#module-nets.datasets.mnist" title="Permalink to this headline">¶</a></h2>
<p>Defines and pre-process the MNIST dataset. The data will be converted into <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> objects.</p>
<dl class="class">
<dt id="nets.datasets.mnist.MNIST">
<em class="property">class </em><code class="sig-prename descclassname">nets.datasets.mnist.</code><code class="sig-name descname">MNIST</code><span class="sig-paren">(</span><em class="sig-param">path_data</em>, <em class="sig-param">path_label</em>, <em class="sig-param">transform=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/datasets/mnist.html#MNIST"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.datasets.mnist.MNIST" title="Permalink to this definition">¶</a></dt>
<dd><p>Loads training, validation, and test partitions of the mnist dataset
(<a class="reference external" href="http://yann.lecun.com/exdb/mnist/">http://yann.lecun.com/exdb/mnist/</a>). If the data is not already contained in data_dir, it will
try to download it.</p>
<p>This dataset contains 60000 training examples, and 10000 test examples of handwritten digits
in {0, …, 9} and corresponding labels. Each handwritten image has an “original” dimension of
28x28x1, and is stored row-wise as a string of 784x1 bytes. Pixel values are in range 0 to 255
(inclusive).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data_dir</strong> – String. Relative or absolute path of the dataset.</p></li>
<li><p><strong>devel_size</strong> – Integer. Size of the development (validation) dataset partition.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>float64 numpy array with shape [784, 60000-devel_size] with values in [0, 1].
Y_train: uint8 numpy array with shape [60000-devel_size]. Labels.
X_devel: float64 numpy array with shape [784, devel_size] with values in [0, 1].
Y_devel: uint8 numpy array with shape [devel_size]. Labels.
X_test: float64 numpy array with shape [784, 10000] with values in [0, 1].
Y_test: uint8 numpy array with shape [10000]. Labels.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>X_train</p>
</dd>
</dl>
<dl class="method">
<dt id="nets.datasets.mnist.MNIST.splits">
<em class="property">classmethod </em><code class="sig-name descname">splits</code><span class="sig-paren">(</span><em class="sig-param">root='.data'</em>, <em class="sig-param">train_data='train-images-idx3-ubyte.gz'</em>, <em class="sig-param">train_label='train-labels-idx1-ubyte.gz'</em>, <em class="sig-param">test_data='t10k-images-idx3-ubyte.gz'</em>, <em class="sig-param">test_label='t10k-labels-idx1-ubyte.gz'</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/datasets/mnist.html#MNIST.splits"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.datasets.mnist.MNIST.splits" title="Permalink to this definition">¶</a></dt>
<dd><p>Loads training and test partitions of the [mnist dataset](<a class="reference external" href="https://www.cs.toronto.edu/~kriz/cifar.html">https://www.cs.toronto.edu/~kriz/cifar.html</a>). If
the data is not already contained in the <code class="docutils literal notranslate"><span class="pre">root</span></code> folder, it will download it.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>root</strong> (<em>str</em>) – relative or absolute path of the dataset.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>training and testing datasets</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>tuple(<a class="reference internal" href="#nets.data.dataset.Dataset" title="nets.data.dataset.Dataset">Dataset</a>)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-nets.datasets.svhn">
<span id="nets-datasets-svhn"></span><h2>nets.datasets.svhn<a class="headerlink" href="#module-nets.datasets.svhn" title="Permalink to this headline">¶</a></h2>
</div>
</div>
<div class="section" id="nets-utils">
<h1>nets.utils<a class="headerlink" href="#nets-utils" title="Permalink to this headline">¶</a></h1>
<div class="section" id="module-nets.utils.display">
<span id="nets-utils-display"></span><h2>nets.utils.display<a class="headerlink" href="#module-nets.utils.display" title="Permalink to this headline">¶</a></h2>
<p>This modules defines basic function to render a simulation, like progress bar and statistics table.</p>
<dl class="function">
<dt id="nets.utils.display.describe_stats">
<code class="sig-prename descclassname">nets.utils.display.</code><code class="sig-name descname">describe_stats</code><span class="sig-paren">(</span><em class="sig-param">state_dict</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/utils/display.html#describe_stats"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.utils.display.describe_stats" title="Permalink to this definition">¶</a></dt>
<dd><p>Describe and render a dictionary. Usually, this function is called on a <code class="docutils literal notranslate"><span class="pre">Solver</span></code> state dictionary,
and merged with a progress bar.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>state_dict</strong> (<em>dict</em>) – the dictionary to showcase</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the dictionary to render.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>string</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="nets.utils.display.get_time">
<code class="sig-prename descclassname">nets.utils.display.</code><code class="sig-name descname">get_time</code><span class="sig-paren">(</span><em class="sig-param">start_time</em>, <em class="sig-param">end_time</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/utils/display.html#get_time"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.utils.display.get_time" title="Permalink to this definition">¶</a></dt>
<dd><p>Get ellapsed time in minutes and seconds.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>start_time</strong> (<em>float</em>) – strarting time</p></li>
<li><p><strong>end_time</strong> (<em>float</em>) – ending time</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>elapsed time in minutes
elapsed_secs (float): elapsed time in seconds.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>elapsed_mins (float)</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="nets.utils.display.progress_bar">
<code class="sig-prename descclassname">nets.utils.display.</code><code class="sig-name descname">progress_bar</code><span class="sig-paren">(</span><em class="sig-param">current_index</em>, <em class="sig-param">max_index</em>, <em class="sig-param">prefix=None</em>, <em class="sig-param">suffix=None</em>, <em class="sig-param">start_time=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/utils/display.html#progress_bar"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.utils.display.progress_bar" title="Permalink to this definition">¶</a></dt>
<dd><p>Display a progress bar and duration.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>current_index</strong> (<em>int</em>) – current state index (or epoch number).</p></li>
<li><p><strong>max_index</strong> (<em>int</em>) – maximal numbers of state.</p></li>
<li><p><strong>prefix</strong> (<em>str</em><em>, </em><em>optional</em>) – prefix of the progress bar. The default is None.</p></li>
<li><p><strong>suffix</strong> (<em>str</em><em>, </em><em>optional</em>) – suffix of the progress bar. The default is None.</p></li>
<li><p><strong>start_time</strong> (<em>float</em><em>, </em><em>optional</em>) – starting time of the progress bar. If not None, it will display the time
spent from the beginning to the current state. The default is None.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>None. Display the progress bar in the console.</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="module-nets.utils.errors">
<span id="nets-utils-errors"></span><h2>nets.utils.errors<a class="headerlink" href="#module-nets.utils.errors" title="Permalink to this headline">¶</a></h2>
<p>Defines custom errors.</p>
<dl class="exception">
<dt id="nets.utils.errors.BackwardCallError">
<em class="property">exception </em><code class="sig-prename descclassname">nets.utils.errors.</code><code class="sig-name descname">BackwardCallError</code><a class="reference internal" href="../_modules/nets/utils/errors.html#BackwardCallError"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.utils.errors.BackwardCallError" title="Permalink to this definition">¶</a></dt>
<dd><p>This error is used when the call to <code class="docutils literal notranslate"><span class="pre">backward</span></code> method is not legit.</p>
</dd></dl>

</div>
<div class="section" id="module-nets.utils.functions">
<span id="nets-utils-functions"></span><h2>nets.utils.functions<a class="headerlink" href="#module-nets.utils.functions" title="Permalink to this headline">¶</a></h2>
<p>Utility functions.</p>
<dl class="function">
<dt id="nets.utils.functions.append2dict">
<code class="sig-prename descclassname">nets.utils.functions.</code><code class="sig-name descname">append2dict</code><span class="sig-paren">(</span><em class="sig-param">dict1</em>, <em class="sig-param">*dicts</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nets/utils/functions.html#append2dict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nets.utils.functions.append2dict" title="Permalink to this definition">¶</a></dt>
<dd><p>Append key values to another dict with the same keys.</p>
<p>Args:
dict1 (dict): dictionary where values will be added.
dict2 (dict) dictionaries to extract values and append to another one.</p>
<blockquote>
<div><p>This dictionary should have the same keys as dict1.</p>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>None</p>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">dict1</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;key1&quot;</span><span class="p">:</span> <span class="p">[],</span> <span class="s2">&quot;key2&quot;</span><span class="p">:</span> <span class="p">[]}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dict2</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;key1&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;key1&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">append2dict</span><span class="p">(</span><span class="n">dict1</span><span class="p">,</span> <span class="n">dict2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dict1</span>
<span class="go">    {&quot;key1&quot;: [0], &quot;key2&quot;: [1]}</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">dict3</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;key1&quot;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="s2">&quot;key1&quot;</span><span class="p">:</span> <span class="mi">3</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dict4</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;key1&quot;</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span> <span class="s2">&quot;key1&quot;</span><span class="p">:</span> <span class="mi">5</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">append2dict</span><span class="p">(</span><span class="n">dict1</span><span class="p">,</span> <span class="n">dict3</span><span class="p">,</span> <span class="n">dict4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dict1</span>
<span class="go">    {&quot;key1&quot;: [0, 2, 4], &quot;key2&quot;: [1, 3, 5]}</span>
</pre></div>
</div>
</dd></dl>

</div>
</div>


             </article>
             
            </div>
            <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
      
        <a href="tutorial-cnn.html" class="btn btn-neutral" title="Convolutional Layers" accesskey="p" rel="prev"><img src="../_static/images/chevron-right-orange.svg" class="previous-page"> Previous</a>
      
    </div>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2020, Arthur Dujardin.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">nets</a></li>
<li><a class="reference internal" href="#id1">nets</a><ul>
<li><a class="reference internal" href="#module-nets.tensor">nets.tensor</a></li>
<li><a class="reference internal" href="#module-nets.numeric">nets.numeric</a></li>
<li><a class="reference internal" href="#module-nets.functional">nets.functional</a></li>
</ul>
</li>
<li><a class="reference internal" href="#nets-autograd">nets.autograd</a><ul>
<li><a class="reference internal" href="#module-nets.autograd.functions">nets.autograd.functions</a></li>
<li><a class="reference internal" href="#module-nets.autograd.hook">nets.autograd.hook</a></li>
<li><a class="reference internal" href="#module-nets.autograd.numeric">nets.autograd.numeric</a></li>
<li><a class="reference internal" href="#module-nets.autograd.ops">nets.autograd.ops</a></li>
<li><a class="reference internal" href="#module-nets.autograd.parameter">nets.autograd.parameter</a></li>
</ul>
</li>
<li><a class="reference internal" href="#nets-nn">nets.nn</a><ul>
<li><a class="reference internal" href="#module-nets.nn.activation">nets.nn.activation</a></li>
<li><a class="reference internal" href="#module-nets.nn.functional">nets.nn.functional</a></li>
<li><a class="reference internal" href="#module-nets.nn.loss">nets.nn.loss</a></li>
<li><a class="reference internal" href="#module-nets.nn.utils">nets.nn.utils</a></li>
</ul>
</li>
<li><a class="reference internal" href="#nets-nn-modules">nets.nn.modules</a><ul>
<li><a class="reference internal" href="#module-nets.nn.modules.conv">nets.nn.conv</a></li>
<li><a class="reference internal" href="#module-nets.nn.modules.dnn">nets.nn.dnn</a></li>
<li><a class="reference internal" href="#module-nets.nn.modules.dropout">nets.nn.dropout</a></li>
<li><a class="reference internal" href="#module-nets.nn.modules.linear">nets.nn.linear</a></li>
<li><a class="reference internal" href="#module-nets.nn.modules.module">nets.nn.module</a></li>
<li><a class="reference internal" href="#module-nets.nn.modules.pool">nets.nn.pool</a></li>
<li><a class="reference internal" href="#module-nets.nn.modules.rnn">nets.nn.rnn</a></li>
<li><a class="reference internal" href="#module-nets.nn.modules.rnnbase">nets.nn.rnnbase</a></li>
<li><a class="reference internal" href="#module-nets.nn.modules.sequential">nets.nn.sequential</a></li>
</ul>
</li>
<li><a class="reference internal" href="#nets-optim">nets.optim</a><ul>
<li><a class="reference internal" href="#module-nets.optim.optimizer">nets.optim.optimizer</a></li>
<li><a class="reference internal" href="#module-nets.optim.adagrad">nets.optim.adagrad</a></li>
<li><a class="reference internal" href="#module-nets.optim.adam">nets.optim.adam</a></li>
<li><a class="reference internal" href="#module-nets.optim.rmsprop">nets.optim.rmsprop</a></li>
<li><a class="reference internal" href="#module-nets.optim.sgd">nets.optim.sgd</a></li>
</ul>
</li>
<li><a class="reference internal" href="#nets-solver">nets.solver</a><ul>
<li><a class="reference internal" href="#module-nets.solver.solver">nets.solver.solver</a></li>
</ul>
</li>
<li><a class="reference internal" href="#nets-data">nets.data</a><ul>
<li><a class="reference internal" href="#module-nets.data.batch">nets.data.batch</a></li>
<li><a class="reference internal" href="#module-nets.data.dataset">nets.data.dataset</a></li>
<li><a class="reference internal" href="#module-nets.data.example">nets.data.example</a></li>
<li><a class="reference internal" href="#module-nets.data.iterator">nets.data.iterator</a></li>
</ul>
</li>
<li><a class="reference internal" href="#nets-datasets">nets.datasets</a><ul>
<li><a class="reference internal" href="#module-nets.datasets.cifar">nets.datasets.cifar</a></li>
<li><a class="reference internal" href="#module-nets.datasets.mnist">nets.datasets.mnist</a></li>
<li><a class="reference internal" href="#module-nets.datasets.svhn">nets.datasets.svhn</a></li>
</ul>
</li>
<li><a class="reference internal" href="#nets-utils">nets.utils</a><ul>
<li><a class="reference internal" href="#module-nets.utils.display">nets.utils.display</a></li>
<li><a class="reference internal" href="#module-nets.utils.errors">nets.utils.errors</a></li>
<li><a class="reference internal" href="#module-nets.utils.functions">nets.utils.functions</a></li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
         <script src="../_static/jquery.js"></script>
         <script src="../_static/underscore.js"></script>
         <script src="../_static/doctools.js"></script>
         <script src="../_static/language_data.js"></script>
         <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
     

  

  <script type="text/javascript" src="../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../_static/js/vendor/bootstrap.min.js"></script>
  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for Nets</p>
          <a class="with-right-arrow" href="https://arthurdjn.github.io/nets/">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get beginners tutorials and create state-of-the-art models</p>
          <a class="with-right-arrow" href="https://arthurdjn.github.io/nets/source/tutorial.html">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Check the GitHub page and contribute to the project</p>
          <a class="with-right-arrow" href="https://github.com/arthurdjn/nets">View GitHub</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://arthurdjn.github.io/nets/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://arthurdjn.github.io/nets/">PyTorch</a></li>
            <li><a href="https://arthurdjn.github.io/nets/source/getting-started.html">Get Started</a></li>
            <li><a href="https://arthurdujardin.com/project/nets.html">Blog</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li><a href="https://arthurdjn.github.io/nets/source/tutorial.html">Tutorials</a></li>
            <li><a href="https://arthurdjn.github.io/nets/">Docs</a></li>
            <li><a href="https://github.com/arthurdjn/nets/issues" target="_blank">Github Issues</a></li>
          </ul>
        </div>

      </div>
    </div>
  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://arthurdjn.github.io/nets/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://arthurdjn.github.io/nets/source/getting-started.html">Get Started</a>
          </li>


          <li>
            <a href="https://arthurdujardin.com/project/nets.html">Blog</a>
          </li>

          <li>
            <a href="https://arthurdjn.github.io/nets/source/tutorial.html">Tutorials</a>
          </li>

          <li>
            <a href="https://arthurdjn.github.io/nets/">Docs</a>
          </li>

          <li>
            <a href="https://github.com/arthurdjn/nets">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>