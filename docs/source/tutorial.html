


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Tensor &mdash; nets 0.0.2 documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../_static/pygments.css" type="text/css" /> -->
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="XOR" href="tutorial-xor.html" />
    <link rel="prev" title="About" href="getting-started.html" /> 

  
  <script src="../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://arthurdjn.github.io/nets/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://arthurdjn.github.io/nets/source/getting-started.html">Get Started</a>
          </li>

          <li>
            <a href="https://arthurdujardin.com/project/nets.html">Blog</a>
          </li>

          <li>
            <a href="https://arthurdjn.github.io/nets/source/tutorial.html">Tutorials</a>
          </li>

          <li>
            <a href="https://arthurdjn.github.io/nets/">Docs</a>
          </li>

          <li>
            <a href="https://github.com/arthurdjn/nets">Github</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>


<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            

            
              
              
            

            


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

            
          </div>

          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="getting-started.html">About</a></li>
<li class="toctree-l1"><a class="reference internal" href="getting-started.html#instalation">Instalation</a></li>
<li class="toctree-l1"><a class="reference internal" href="getting-started.html#usage">Usage</a></li>
</ul>
<p class="caption"><span class="caption-text">Tutorial</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorial-xor.html">XOR</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorial-cnn.html">Convolutional Layers</a></li>
</ul>
<p class="caption"><span class="caption-text">Package</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="package.html">nets</a></li>
<li class="toctree-l1"><a class="reference internal" href="package.html#id1">nets</a></li>
<li class="toctree-l1"><a class="reference internal" href="package.html#nets-autograd">nets.autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="package.html#nets-nn">nets.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="package.html#nets-nn-modules">nets.nn.modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="package.html#nets-optim">nets.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="package.html#nets-solver">nets.solver</a></li>
<li class="toctree-l1"><a class="reference internal" href="package.html#nets-data">nets.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="package.html#nets-datasets">nets.datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="package.html#nets-utils">nets.utils</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
      <li>Tensor</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="../_sources/source/tutorial.rst.txt" rel="nofollow"><img src="../_static/images/view-page-source-icon.svg"></a>
          
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <p>Open this tutorial in a jupyter <a class="reference external" href="https://github.com/arthurdjn/nets/blob/master/0_Getting_Started_with_NETS.ipynb">notebook</a>.</p>
<div class="section" id="tensor">
<h1>Tensor<a class="headerlink" href="#tensor" title="Permalink to this headline">¶</a></h1>
<div class="section" id="definition">
<h2>Definition<a class="headerlink" href="#definition" title="Permalink to this headline">¶</a></h2>
<p>A tensor is multi-dimensional array, similar to NumPy arrays.
Their particularity is their capability to stock previous gradients and operations.
Their architecture was highly inspired from <em>PyTorch</em> documentation.</p>
<p>In <em>PyTorch</em>, you create tensors using the tensor built-in method <code class="docutils literal notranslate"><span class="pre">tensor</span></code> or use <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> class directly (it is not recommended to instantiate a tensor this way however).
<em>NETS</em> does not have (yet) built-in functions, so you will need to use the <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> class to create a tensor.</p>
<p>Tensors supports basic mathematical operations.
All these operations are defined in the <code class="docutils literal notranslate"><span class="pre">nets/ops.py</span></code> module. Here is a list of some operations currently supported:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">add</span></code> which add to tensors (broadcasting is supported),</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">subtract</span></code> which subtract to tensors (broadcasting is supported),</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">multiply</span></code> which add to tensors with the same shape,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dot</span></code> which compute the dot product <code class="docutils literal notranslate"><span class="pre">&#64;</span></code> of two matrices,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">exp</span></code> which compute the element-wise exponentiation of a tensor.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">sum</span></code> which sum up all elements in a tensor,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">transpose</span></code> which transpose a tensor.</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">nets</span>

<span class="n">t1</span> <span class="o">=</span> <span class="n">nets</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
                  <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">]])</span>
<span class="n">t2</span> <span class="o">=</span> <span class="n">nets</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span>
                  <span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="mi">8</span><span class="p">]])</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;t1 =</span><span class="se">\n</span><span class="si">{t1}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;t2 =</span><span class="se">\n</span><span class="si">{t2}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Some basic operations&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;t1 + t2: </span><span class="se">\n</span><span class="s2">{t1 + t2}&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;t1 - t2: </span><span class="se">\n</span><span class="s2">{t1 - t2}&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;t1 * t2: </span><span class="se">\n</span><span class="s2">{t1 *t2}&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;t1 @ t2: </span><span class="se">\n</span><span class="s2">{t1 @ t2}&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;t1 ** 2: </span><span class="se">\n</span><span class="s2">{t1 ** 2}&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;t1 / t2: </span><span class="se">\n</span><span class="s2">{t1 / t2}&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;t1 / 10: </span><span class="se">\n</span><span class="s2">{t1 / 10}&quot;</span><span class="p">)</span>
</pre></div>
</div>
<blockquote class="sphx-glr-script-out">
<div><p>Out:</p>
</div></blockquote>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>t1 =
Tensor([[1, 3],
        [5, 7]])
t2 =
Tensor([[2, 4],
        [6, 8]])

Some basic operations
t1 + t2:
Tensor([[ 3,  7],
        [11, 15]])
t1 - t2:
Tensor([[-1, -1],
        [-1, -1]])
t1 * t2:
Tensor([[ 2, 12],
        [30, 56]])
t1 @ t2:
Tensor([[20, 28],
        [52, 76]])
t1 ** 2:
Tensor([[ 1,  9],
        [25, 49]])
t1 / t2:
Tensor([[0.5000, 0.7500],
        [0.8333, 0.8750]])
t1 / 10:
Tensor([[0.1, 0.3],
        [0.5, 0.7]])
</pre></div>
</div>
</div>
<div class="section" id="gradients">
<h2>Gradients<a class="headerlink" href="#gradients" title="Permalink to this headline">¶</a></h2>
<p><em>NETS</em> uses a custom <em>autograd</em> system, made with numpy.
Some vanilla architectures do not depends on this functionality however, like <code class="docutils literal notranslate"><span class="pre">DNN</span></code> networks.
All these information will be detailed in the model’s section.</p>
<p>As you may have seen, there is a <code class="docutils literal notranslate"><span class="pre">requires_grad</span></code> set to <code class="docutils literal notranslate"><span class="pre">False</span></code> by default when we create a tensor.
This attribute attributes works similarly as <strong>PyTorch</strong>’s attribute. If set to <code class="docutils literal notranslate"><span class="pre">True</span></code>,
previous gradients will be registered and saved in this tensor, in the <code class="docutils literal notranslate"><span class="pre">_hooks</span></code> attribute.
This attribute is basically a list containing all previous gradients.
That is, when calling the <code class="docutils literal notranslate"><span class="pre">backward</span></code> method on this tensor with an upstream gradient,
it will propagate through all previous gradients.</p>
<p>Let’s see some basic examples:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">t1</span> <span class="o">=</span> <span class="n">nets</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">t2</span> <span class="o">=</span> <span class="n">nets</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Some operations</span>
<span class="n">t3</span> <span class="o">=</span> <span class="n">t1</span> <span class="o">+</span> <span class="n">t2</span> <span class="o">+</span> <span class="mi">4</span>
<span class="n">t4</span> <span class="o">=</span> <span class="n">t3</span> <span class="o">*</span> <span class="n">t2</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;t1 =</span><span class="se">\n</span><span class="si">{t1}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;t2 =</span><span class="se">\n</span><span class="si">{t2}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Operation:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;t3 = t1 + t2 + 4&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;t4 = t3 * t2&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">t3 =</span><span class="se">\n</span><span class="si">{t3}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;t4 =</span><span class="se">\n</span><span class="si">{t4}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Before backpropagation&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;t1 gradient: </span><span class="si">{t1.grad}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;t2 gradient: </span><span class="si">{t2.grad}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;t3 gradient: </span><span class="si">{t3.grad}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;t4 gradient: </span><span class="si">{t4.grad}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Upstream gradient</span>
<span class="n">grad</span> <span class="o">=</span> <span class="n">nets</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>

<span class="c1"># Back-propagation</span>
<span class="n">t4</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">grad</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">After backpropagation&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;t1 gradient: </span><span class="si">{t1.grad}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;t2 gradient: </span><span class="si">{t2.grad}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;t3 gradient: </span><span class="si">{t3.grad}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;t4 gradient: </span><span class="si">{t4.grad}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<blockquote class="sphx-glr-script-out">
<div><p>Out:</p>
</div></blockquote>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>t1 =
Tensor([1, 3], requires_grad=True)
t2 =
Tensor([2, 4], requires_grad=True)

Operation:
t3 = t1 + t2 + 4
t4 = t3 * t2

t3 =
Tensor([ 7, 11], requires_grad=True)
t4 =
Tensor([14, 44], requires_grad=True)

Before backpropagation
t1 gradient: Tensor([0., 0.])
t2 gradient: Tensor([0., 0.])
t3 gradient: Tensor([0., 0.])
t4 gradient: Tensor([0., 0.])

After backpropagation
t1 gradient: Tensor([-2.,  8.])
t2 gradient: Tensor([-9., 30.])
t3 gradient: Tensor([-2.,  8.])
t4 gradient: Tensor([-1.,  2.])
</pre></div>
</div>
</div>
<div class="section" id="autograd">
<h2>Autograd<a class="headerlink" href="#autograd" title="Permalink to this headline">¶</a></h2>
<p>The autograd system creates a computational graph dynamically, used to update deep learning models.</p>
<p>The backward pass is called for computing the gradients and create a computational graph from all previous operations.
This backward pass can be called using the <code class="docutils literal notranslate"><span class="pre">backward</span></code> method of a <code class="docutils literal notranslate"><span class="pre">Module</span></code>.
In this case, you will need to write the computational on your own, as the gradients’ equations depends on a model.
This <em>Vanilla</em> back-propagation is implemented for standard models, like <code class="docutils literal notranslate"><span class="pre">DNN</span></code>, <code class="docutils literal notranslate"><span class="pre">CNN</span></code> and <code class="docutils literal notranslate"><span class="pre">RNN</span></code>.
However, you won’t be able to mix them using this technique unless you change some parts of the <code class="docutils literal notranslate"><span class="pre">backward</span></code> method.</p>
<p>Or, you can use the <em>autograd</em> system. As <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> keeps track of the computational graph in their <code class="docutils literal notranslate"><span class="pre">hooks</span></code>,
it is easier to compute the back-propagation. The back-propagation is decomposed in elementary operations (+, -, /, *, exp)
and the gradient given from a <code class="docutils literal notranslate"><span class="pre">Loss</span></code> function is then transferred through the computational graph.</p>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">t1</span> <span class="o">=</span> <span class="n">nets</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">],</span>
                   <span class="p">[</span><span class="mf">5.</span><span class="p">,</span> <span class="mf">7.</span><span class="p">]],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">t2</span> <span class="o">=</span> <span class="n">nets</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="mf">2.</span><span class="p">,</span> <span class="mf">4.</span><span class="p">],</span>
                   <span class="p">[</span><span class="mf">6.</span><span class="p">,</span> <span class="mf">8.</span><span class="p">]],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Some operations</span>
<span class="n">t3</span> <span class="o">=</span> <span class="n">nets</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">t1</span> <span class="o">*</span> <span class="n">t2</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;t1 =</span><span class="se">\n</span><span class="si">{t1}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;t2 =</span><span class="se">\n</span><span class="si">{t2}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Before backpropagation&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;t1 gradient:</span><span class="se">\n</span><span class="si">{t1.grad}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;t2 gradient:</span><span class="se">\n</span><span class="si">{t2.grad}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;t3 = tanh(t1 * t2)&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;t3 gradient:</span><span class="se">\n</span><span class="si">{t3.grad}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Upstream gradient</span>
<span class="n">grad</span> <span class="o">=</span> <span class="n">nets</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="o">-</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">],</span>
                    <span class="p">[</span><span class="o">-</span><span class="mf">3.</span><span class="p">,</span> <span class="mf">4.</span><span class="p">]])</span>

<span class="c1"># Back-propagation</span>
<span class="n">t3</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">grad</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">After backpropagation&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;t1 gradient:</span><span class="se">\n</span><span class="si">{t1.grad}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;t2 gradient:</span><span class="se">\n</span><span class="si">{t2.grad}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;t3 = tanh(t1 * t2)&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;t3 gradient:</span><span class="se">\n</span><span class="si">{t3.grad}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<blockquote class="sphx-glr-script-out">
<div><p>Out:</p>
</div></blockquote>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>t1 =
Tensor([[1., 3.],
        [5., 7.]], requires_grad=True)
t2 =
Tensor([[2., 4.],
        [6., 8.]], requires_grad=True)

Before backpropagation
t1 gradient:
Tensor([[0., 0.],
        [0., 0.]])
t2 gradient:
Tensor([[0., 0.],
        [0., 0.]])
t3 = tanh(t1 * t2)
t3 gradient:
Tensor([[0., 0.],
        [0., 0.]])

After backpropagation
t1 gradient:
Tensor([[-1.4130e-01, -6.0402e-10],
        [ 0.0000e+00,  0.0000e+00]])
t2 gradient:
Tensor([[-7.0651e-02, -4.5302e-10],
        [ 0.0000e+00,  0.0000e+00]])
t3 = tanh(t1 * t2)
t3 gradient:
Tensor([[-1., -1.],
        [ 2.,  2.]])
</pre></div>
</div>
</div>
<div class="section" id="example">
<h2>Example<a class="headerlink" href="#example" title="Permalink to this headline">¶</a></h2>
<p>With the backward pass, you can minimize any basic functions (functions that can be decomposed ito elementary operations).
As the <em>autograd</em> system, record the gradients, you can determine the slope at this point and adjust your inputs by a coefficient, called learning rate <span class="math notranslate nohighlight">\(l_r\)</span>.</p>
<p>This is maybe a lot of information and a lot of text, so let’s try to visualize how we can use gradients to minimize a function instead.</p>
<p>The function we will try to minimize is the so called <em>three hump camel</em> function:</p>
<div class="math notranslate nohighlight">
\[\forall x_1, x_2 \in \mathbb{R}, \quad f(x_1, x_2) = 2 x_1^2 - 1.05 x_1^4 + \frac{x_1^6}{6} + x_1 x_2 + x_2^2\]</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">mpl_toolkits.mplot3d</span> <span class="kn">import</span> <span class="n">Axes3D</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">gca</span><span class="p">(</span><span class="n">projection</span><span class="o">=</span><span class="s1">&#39;3d&#39;</span><span class="p">)</span>

<span class="c1"># Make data</span>
<span class="n">camel</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">:</span> <span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">-</span> <span class="mf">1.05</span><span class="o">*</span><span class="n">x</span><span class="o">**</span><span class="mi">4</span> <span class="o">+</span> <span class="p">((</span><span class="n">x</span><span class="o">**</span><span class="mi">6</span><span class="p">)</span><span class="o">/</span><span class="mi">6</span><span class="p">)</span> <span class="o">+</span> <span class="n">x</span><span class="o">*</span><span class="n">y</span> <span class="o">+</span> <span class="n">y</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
<span class="n">Z</span> <span class="o">=</span> <span class="n">camel</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>

<span class="c1"># Plot the surface</span>
<span class="n">ax</span><span class="o">.</span><span class="n">view_init</span><span class="p">(</span><span class="mi">30</span><span class="p">,</span> <span class="mi">105</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot_surface</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;coolwarm&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
<img alt="../_images/camelfunc.png" src="../_images/camelfunc.png" />
<p>Let’s minimize this function with <em>NETS</em></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">random</span> <span class="k">as</span> <span class="nn">rd</span>
<span class="kn">from</span> <span class="nn">nets.optim</span> <span class="kn">import</span> <span class="n">SGD</span>

<span class="c1"># Random x points</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">rd</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">xn</span> <span class="o">=</span> <span class="n">nets</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="c1"># Random y points</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">rd</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">yn</span> <span class="o">=</span> <span class="n">nets</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="c1"># Learning rate</span>
<span class="n">lr</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">SGD</span><span class="p">([</span><span class="n">xn</span><span class="p">,</span> <span class="n">yn</span><span class="p">],</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="c1"># Keep track of the loss</span>
<span class="n">nets_history</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">nets_points_history</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># Run the simulation 50 times</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">50</span><span class="p">):</span>
    <span class="c1"># The gradients accumulates, we need to clear that at each epochs</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="c1"># outputs, can be seen as &quot;predictions&quot;</span>
    <span class="n">zn</span> <span class="o">=</span> <span class="n">camel</span><span class="p">(</span><span class="n">xn</span><span class="p">,</span> <span class="n">yn</span><span class="p">)</span>
    <span class="c1"># Compute the loss (sum of all values)</span>
    <span class="c1"># As the minimum is at (0, 0), the lower the loss is, the closest we are to this minimum</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">zn</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>  <span class="c1"># is a 0-tensor</span>
    <span class="c1"># Get the gradients</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="c1"># Update the points</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="c1"># Add the loss to the history and display the current loss in the console</span>
    <span class="n">nets_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
    <span class="n">nets_points_history</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">xn</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">yn</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">zn</span><span class="o">.</span><span class="n">item</span><span class="p">()))</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\r</span><span class="s2">epoch: </span><span class="si">{i:4d}</span><span class="s2"> | loss: {loss.item():1.2E}&quot;</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
</pre></div>
</div>
<blockquote class="sphx-glr-script-out">
<div><p>Out:</p>
</div></blockquote>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>epoch:   49 | loss: 3.61E-09
</pre></div>
</div>
</div>
</div>


             </article>
             
            </div>
            <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="tutorial-xor.html" class="btn btn-neutral float-right" title="XOR" accesskey="n" rel="next">Next <img src="../_static/images/chevron-right-orange.svg" class="next-page"></a>
      
      
        <a href="getting-started.html" class="btn btn-neutral" title="About" accesskey="p" rel="prev"><img src="../_static/images/chevron-right-orange.svg" class="previous-page"> Previous</a>
      
    </div>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2020, Arthur Dujardin.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">Tensor</a><ul>
<li><a class="reference internal" href="#definition">Definition</a></li>
<li><a class="reference internal" href="#gradients">Gradients</a></li>
<li><a class="reference internal" href="#autograd">Autograd</a></li>
<li><a class="reference internal" href="#example">Example</a></li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
         <script src="../_static/jquery.js"></script>
         <script src="../_static/underscore.js"></script>
         <script src="../_static/doctools.js"></script>
         <script src="../_static/language_data.js"></script>
         <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
     

  

  <script type="text/javascript" src="../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../_static/js/vendor/bootstrap.min.js"></script>
  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for Nets</p>
          <a class="with-right-arrow" href="https://arthurdjn.github.io/nets/">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get beginners tutorials and create state-of-the-art models</p>
          <a class="with-right-arrow" href="https://arthurdjn.github.io/nets/source/tutorial.html">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Check the GitHub page and contribute to the project</p>
          <a class="with-right-arrow" href="https://github.com/arthurdjn/nets">View GitHub</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://arthurdjn.github.io/nets/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://arthurdjn.github.io/nets/">NETS</a></li>
            <li><a href="https://arthurdjn.github.io/nets/source/getting-started.html">Get Started</a></li>
            <li><a href="https://arthurdujardin.com/project/nets.html">Blog</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li><a href="https://arthurdjn.github.io/nets/source/tutorial.html">Tutorials</a></li>
            <li><a href="https://arthurdjn.github.io/nets/">Docs</a></li>
            <li><a href="https://github.com/arthurdjn/nets/issues" target="_blank">Github Issues</a></li>
          </ul>
        </div>

      </div>
    </div>
  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://arthurdjn.github.io/nets/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://arthurdjn.github.io/nets/source/getting-started.html">Get Started</a>
          </li>


          <li>
            <a href="https://arthurdujardin.com/project/nets.html">Blog</a>
          </li>

          <li>
            <a href="https://arthurdjn.github.io/nets/source/tutorial.html">Tutorials</a>
          </li>

          <li>
            <a href="https://arthurdjn.github.io/nets/">Docs</a>
          </li>

          <li>
            <a href="https://github.com/arthurdjn/nets">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>